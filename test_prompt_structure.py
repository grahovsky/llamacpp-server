#!/usr/bin/env python3

import asyncio
from llamacpp_server.prompts.service import PromptService

async def test_prompt_structure():
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–º–ø—Ç–∞...")
    
    ps = PromptService()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ RAG –ø—Ä–æ–º–ø—Ç–∞
    query = "–∫–∞–∫ –ø–µ—Ä–µ–π—Ç–∏ —Å okd –Ω–∞ deckhouse?"
    context = [
        "–î–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ —Å OKD –Ω–∞ Deckhouse –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ:",
        "1. –°–æ–∑–¥–∞—Ç—å –æ–±—Ä–∞—â–µ–Ω–∏–µ –≤ –ï–°–û",
        "2. –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –º–∏–≥—Ä–∞—Ü–∏—é –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π"
    ]
    
    prompt = await ps.create_rag_prompt(query, context)
    
    print("‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–º–ø—Ç–∞:")
    print("=" * 60)
    print(prompt)
    print("=" * 60)
    
    print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∏:")
    print(f"‚Ä¢ –°–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–∫–µ–Ω—ã Llama 3.1: {'‚úÖ' if '<|begin_of_text|>' in prompt else '‚ùå'}")
    print(f"‚Ä¢ –°–æ–¥–µ—Ä–∂–∏—Ç system —Ä–æ–ª—å: {'‚úÖ' if '<|start_header_id|>system<|end_header_id|>' in prompt else '‚ùå'}")
    print(f"‚Ä¢ –°–æ–¥–µ—Ä–∂–∏—Ç user —Ä–æ–ª—å: {'‚úÖ' if '<|start_header_id|>user<|end_header_id|>' in prompt else '‚ùå'}")
    print(f"‚Ä¢ –°–æ–¥–µ—Ä–∂–∏—Ç assistant —Ä–æ–ª—å: {'‚úÖ' if '<|start_header_id|>assistant<|end_header_id|>' in prompt else '‚ùå'}")
    expected_ending = 'assistant<|end_header_id|>\n\n'
    print(f"‚Ä¢ –ó–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ: {'‚úÖ' if prompt.endswith(expected_ending) else '‚ùå'}")
    
    print(f"\nüìè –†–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤")

if __name__ == "__main__":
    asyncio.run(test_prompt_structure()) 