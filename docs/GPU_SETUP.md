# üöÄ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –¥–ª—è llamacpp-server

## –í–∞—Ä–∏–∞–Ω—Ç—ã —É—Å—Ç–∞–Ω–æ–≤–∫–∏

### üñ•Ô∏è CPU –≤–µ—Ä—Å–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
```bash
uv sync --extra cpu
```

### üéÆ GPU –≤–µ—Ä—Å–∏–∏ (–≥–æ—Ç–æ–≤—ã–µ –±–∏–Ω–∞—Ä–Ω–∏–∫–∏)

#### CUDA 12.1
```bash
uv sync --extra cpu
uv add llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
```

#### CUDA 12.2  
```bash
uv sync --extra cpu
uv add llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122
```

#### CUDA 12.3
```bash
uv sync --extra cpu
uv add llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu123
```

#### CUDA 12.4 (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
```bash
uv sync --extra cpu
uv add llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124
```

#### CUDA 12.5 (—Å–∞–º–∞—è –Ω–æ–≤–∞—è)
```bash
uv sync --extra cpu  
uv add llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu125
```

#### Apple Metal (macOS)
```bash
uv sync --extra cpu
uv add llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/metal
```

### üõ†Ô∏è –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å GPU
```bash
uv sync --extra all
```

## –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏

–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç:
```bash
uv run python -c "
import llama_cpp
print('llama-cpp-python version:', llama_cpp.__version__)
from llama_cpp import llama_cpp
gpu_support = hasattr(llama_cpp, 'llama_supports_gpu_offload') and llama_cpp.llama_supports_gpu_offload()
print('CUDA support:', gpu_support)
"
```

## –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è

–ü–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ GPU –≤–µ—Ä—Å–∏–∏ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ:

```bash
# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –Ω–∞ GPU (0 = CPU, -1 = –≤—Å–µ —Å–ª–æ–∏)
export LLAMACPP_N_GPU_LAYERS=20

# ID –æ—Å–Ω–æ–≤–Ω–æ–π GPU (–µ—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ GPU)
export LLAMACPP_MAIN_GPU=0

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏ –º–µ–∂–¥—É GPU (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
export LLAMACPP_TENSOR_SPLIT="0.7,0.3"
```

## –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º

### –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–∏ CUDA
```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–π CUDA
nvidia-smi
nvcc --version
```

### –í—ã–±–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
- **CUDA 12.1** ‚Üí cu121
- **CUDA 12.2** ‚Üí cu122  
- **CUDA 12.3** ‚Üí cu123
- **CUDA 12.4** ‚Üí cu124 (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
- **CUDA 12.5** ‚Üí cu125

### –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
```bash
nvidia-smi
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤
```bash
uv run python -c "
import subprocess
try:
    result = subprocess.run(['nvidia-smi', '-L'], capture_output=True, text=True)
    print('GPU devices:')
    print(result.stdout)
except:
    print('nvidia-smi –Ω–µ –Ω–∞–π–¥–µ–Ω')
"
```

### –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∫–∞
–ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫:
```bash
uv remove llama-cpp-python
# –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≥–æ—Ç–æ–≤—ã–µ –±–∏–Ω–∞—Ä–Ω–∏–∫–∏ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
uv pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu125
# –ò–ª–∏ –∫–æ–º–ø–∏–ª—è—Ü–∏—è (–Ω—É–∂–µ–Ω CUDA Toolkit)
CMAKE_ARGS="-DGGML_CUDA=on" uv pip install llama-cpp-python --force-reinstall --no-binary llama-cpp-python
```

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ CUDA Toolkit (–¥–ª—è –∫–æ–º–ø–∏–ª—è—Ü–∏–∏)
–ï—Å–ª–∏ –≥–æ—Ç–æ–≤—ã–µ –±–∏–Ω–∞—Ä–Ω–∏–∫–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç, –Ω—É–∂–µ–Ω CUDA Toolkit:

#### Arch Linux
```bash
sudo pacman -S cuda cudnn
```

#### Ubuntu/Debian
```bash
# –î–æ–±–∞–≤–∏—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π NVIDIA
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get install cuda-toolkit-12-8
```

## –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞

```bash
# –° GPU (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
LLAMACPP_N_GPU_LAYERS=20 uv run python -m llamacpp_server.main

# –¢–æ–ª—å–∫–æ CPU
LLAMACPP_N_GPU_LAYERS=0 uv run python -m llamacpp_server.main
```

## –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

- **CPU**: ~2-5 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫
- **GPU (RTX 3060)**: ~15-25 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫  
- **GPU (RTX 4090)**: ~50-80 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫

–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –Ω–∞ GPU –≤–ª–∏—è–µ—Ç –Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å:
- `n_gpu_layers=0` - —Ç–æ–ª—å–∫–æ CPU
- `n_gpu_layers=20` - —á–∞—Å—Ç–∏—á–Ω–æ –Ω–∞ GPU
- `n_gpu_layers=-1` - –≤—Å—è –º–æ–¥–µ–ª—å –Ω–∞ GPU

## –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ —Å–∞–º–∞—è —Å–≤–µ–∂–∞—è –≤–µ—Ä—Å–∏—è

### –ù–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ pip —Å –Ω—É–∂–Ω—ã–º–∏ —Ñ–ª–∞–≥–∞–º–∏
uv add "llama-cpp-python[cu12]" --reinstall-package

### –ò–ª–∏ –¥–ª—è —Å–±–æ—Ä–∫–∏ –∏–∑ –∏—Å—Ö–æ–¥–Ω–∏–∫–æ–≤ (–¥–æ–ª–≥–æ, –Ω–æ —Å–≤–µ–∂–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏)
CMAKE_ARGS="-DGGML_CUDA=on" uv add llama-cpp-python --force-reinstall --no-deps