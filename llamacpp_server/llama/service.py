"""–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLama.cpp –≤ RAG-only —Ä–µ–∂–∏–º–µ."""

import time
import uuid
from collections.abc import AsyncIterator

import structlog
from llama_cpp import Llama

from ..config.settings import get_settings
from ..domain.models import (
    ChatCompletionRequest,
    ChatMessage,
    Choice,
    CompletionResponse,
    TextCompletionRequest,
    Usage,
)
from ..retrieval.protocols import RAGServiceProtocol
from ..prompts.protocols import PromptServiceProtocol
from .history_manager import ChatHistoryManager

logger = structlog.get_logger(__name__)


class LlamaService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLama –º–æ–¥–µ–ª—å—é –≤ RAG-only —Ä–µ–∂–∏–º–µ."""

    def __init__(
        self,
        llama: Llama,
        rag_service: RAGServiceProtocol = None,
        prompt_service: PromptServiceProtocol = None
    ) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞."""
        self._llama = llama
        self._rag_service = rag_service
        self._prompt_service = prompt_service
        settings = get_settings()
        
        if not rag_service:
            raise ValueError("RAG —Å–µ—Ä–≤–∏—Å –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è RAG-only —Å–∏—Å—Ç–µ–º—ã")
        
        if not prompt_service:
            raise ValueError("Prompt —Å–µ—Ä–≤–∏—Å –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω")
        
        self.history_manager = ChatHistoryManager(
            llama_model=llama,
            max_tokens=settings.max_history_tokens,
            reserve_tokens=settings.max_response_tokens
        )
        
        logger.info("LlamaService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ RAG-only —Ä–µ–∂–∏–º–µ")

    async def chat_completion(self, request: ChatCompletionRequest) -> CompletionResponse:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è chat completion —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º RAG."""
        logger.info("üß† RAG-only chat completion", model=request.model)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å
        user_query = self._extract_user_query(request.messages)
        
        if not user_query:
            logger.warning("–ù–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞")
            return self._create_error_response("–ù–µ –Ω–∞–π–¥–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å")

        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞", query_preview=user_query[:100])

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ - –æ—Ç–≤–µ—á–∞–µ–º –±–µ–∑ RAG
        if self._is_title_generation_request(user_query):
            logger.info("üìù –û–±–Ω–∞—Ä—É–∂–µ–Ω –∑–∞–ø—Ä–æ—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–µ–∑ RAG")
            return await self._process_simple_completion(request)

        try:
            # –°–æ–∑–¥–∞–µ–º RAG –ø—Ä–æ–º–ø—Ç –¥–ª—è –ª—é–±–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            rag_prompt = await self._rag_service.create_rag_prompt(user_query)
            
            # RAG –ø—Ä–æ–º–ø—Ç —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å RAG –ø—Ä–æ–º–ø—Ç–æ–º
            rag_messages = [
                ChatMessage(role="user", content=rag_prompt)
            ]

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å RAG —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
            rag_request = ChatCompletionRequest(
                messages=rag_messages,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k,
                repeat_penalty=request.repeat_penalty,
                seed=request.seed,
                stream=request.stream
            )

            logger.info("‚úÖ RAG –ø—Ä–æ–º–ø—Ç —Å–æ–∑–¥–∞–Ω", 
                       original_len=len(user_query),
                       rag_len=len(rag_prompt))

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
            return await self._process_completion(rag_request)

        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ RAG –æ–±—Ä–∞–±–æ—Ç–∫–∏", error=str(e), exc_info=True)
            return self._create_error_response(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

    async def chat_completion_stream(
        self, request: ChatCompletionRequest
    ) -> AsyncIterator[dict]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥ chat completion —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º RAG."""
        logger.info("üß† RAG-only streaming chat completion", model=request.model)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å
        user_query = self._extract_user_query(request.messages)
        
        if not user_query:
            logger.warning("–ù–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞")
            yield self._create_stream_error("–ù–µ –Ω–∞–π–¥–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å")
            return

        try:
            # –°–æ–∑–¥–∞–µ–º RAG –ø—Ä–æ–º–ø—Ç
            rag_prompt = await self._rag_service.create_rag_prompt(user_query)
            
            # RAG –ø—Ä–æ–º–ø—Ç —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å RAG –ø—Ä–æ–º–ø—Ç–æ–º
            rag_messages = [
                ChatMessage(role="user", content=rag_prompt)
            ]

            # –°–æ–∑–¥–∞–µ–º RAG –∑–∞–ø—Ä–æ—Å
            rag_request = ChatCompletionRequest(
                messages=rag_messages,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k,
                repeat_penalty=request.repeat_penalty,
                seed=request.seed,
                stream=True
            )

            logger.info("‚úÖ RAG —Å—Ç—Ä–∏–º–∏–Ω–≥ –ø—Ä–æ–º–ø—Ç —Å–æ–∑–¥–∞–Ω")

            # –°—Ç—Ä–∏–º–∏–º –æ—Ç–≤–µ—Ç
            async for chunk in self._process_completion_stream(rag_request):
                yield chunk

        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ RAG —Å—Ç—Ä–∏–º–∏–Ω–≥–∞", error=str(e), exc_info=True)
            yield self._create_stream_error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

    async def text_completion(self, request: TextCompletionRequest) -> CompletionResponse:
        """Text completion —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º RAG."""
        logger.info("üß† RAG-only text completion", model=request.model)

        if not request.prompt:
            return self._create_error_response("–ü—É—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç")

        try:
            # –°–æ–∑–¥–∞–µ–º RAG –ø—Ä–æ–º–ø—Ç
            rag_prompt = await self._rag_service.create_rag_prompt(request.prompt)
            
            # –°–æ–∑–¥–∞–µ–º RAG –∑–∞–ø—Ä–æ—Å
            rag_request = TextCompletionRequest(
                prompt=rag_prompt,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k,
                repeat_penalty=request.repeat_penalty,
                seed=request.seed,
                stream=request.stream
            )

            logger.info("‚úÖ RAG text completion –ø—Ä–æ–º–ø—Ç —Å–æ–∑–¥–∞–Ω")

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
            return await self._process_text_completion(rag_request)

        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ RAG text completion", error=str(e), exc_info=True)
            return self._create_error_response(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

    async def text_completion_stream(
        self, request: TextCompletionRequest
    ) -> AsyncIterator[dict]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥ text completion —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º RAG."""
        logger.info("üß† RAG-only streaming text completion", model=request.model)

        if not request.prompt:
            yield self._create_stream_error("–ü—É—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç")
            return

        try:
            # –°–æ–∑–¥–∞–µ–º RAG –ø—Ä–æ–º–ø—Ç
            rag_prompt = await self._rag_service.create_rag_prompt(request.prompt)
            
            # –°–æ–∑–¥–∞–µ–º RAG –∑–∞–ø—Ä–æ—Å
            rag_request = TextCompletionRequest(
                prompt=rag_prompt,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k,
                repeat_penalty=request.repeat_penalty,
                seed=request.seed,
                stream=True
            )

            logger.info("‚úÖ RAG streaming text completion –ø—Ä–æ–º–ø—Ç —Å–æ–∑–¥–∞–Ω")

            # –°—Ç—Ä–∏–º–∏–º –æ—Ç–≤–µ—Ç
            async for chunk in self._process_text_completion_stream(rag_request):
                yield chunk

        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ RAG text completion —Å—Ç—Ä–∏–º–∏–Ω–≥–∞", error=str(e), exc_info=True)
            yield self._create_stream_error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

    def _extract_user_query(self, messages: list[ChatMessage]) -> str:
        """–ò–∑–≤–ª–µ—á—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å."""
        for msg in reversed(messages):
            if msg.role == "user" and msg.content.strip():
                return msg.content.strip()
        return ""

    async def _process_completion(self, request: ChatCompletionRequest) -> CompletionResponse:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ completion –∑–∞–ø—Ä–æ—Å–∞."""
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–ª—è llama.cpp
        formatted_prompt = self._format_chat_messages(request.messages)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        params = {
            "prompt": formatted_prompt,
            "max_tokens": request.max_tokens or 1024,
            "temperature": request.temperature or 0.7,
            "top_p": request.top_p or 0.9,
            "top_k": request.top_k or 40,
            "repeat_penalty": request.repeat_penalty or 1.05,  # –£–º–µ–Ω—å—à–∞–µ–º –¥–ª—è –º–µ–Ω—å—à–∏—Ö –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
            "seed": request.seed or -1,
            "stream": False,
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è Llama 3.1 - stop —Ç–æ–∫–µ–Ω—ã
            "stop": ["<|eot_id|>", "<|end_of_text|>", "<|im_end|>", "</s>", "–í–æ–ø—Ä–æ—Å:", "useranswer", "–û—Ç–≤–µ—Ç:"],
        }
        
        logger.debug("ü§ñ LLM –ø–∞—Ä–∞–º–µ—Ç—Ä—ã", 
                    prompt_len=len(formatted_prompt),
                    max_tokens=params["max_tokens"],
                    temperature=params["temperature"],
                    top_p=params["top_p"],
                    top_k=params["top_k"],
                    repeat_penalty=params["repeat_penalty"],
                    seed=params["seed"])
        
        logger.debug("üî§ –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM", prompt_preview=formatted_prompt[:500])

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = self._llama.create_completion(**params)
        
        logger.debug("üéØ LLM –æ—Ç–≤–µ—Ç", response_keys=list(response.keys()) if response else None)
        if response and "choices" in response:
            choice = response["choices"][0]
            content = choice.get("message", {}).get("content", choice.get("text", ""))
            logger.debug("üìù –ö–æ–Ω—Ç–µ–Ω—Ç –æ—Ç–≤–µ—Ç–∞", 
                        content_len=len(content), 
                        content_preview=content[:200],
                        finish_reason=choice.get("finish_reason"))

        return self._create_completion_response(response, request.model, "chat.completion")

    async def _process_completion_stream(self, request: ChatCompletionRequest) -> AsyncIterator[dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ streaming completion –∑–∞–ø—Ä–æ—Å–∞."""
        formatted_prompt = self._format_chat_messages(request.messages)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        params = {
            "prompt": formatted_prompt,
            "max_tokens": request.max_tokens or 1024,
            "temperature": request.temperature or 0.7,
            "top_p": request.top_p or 0.9,
            "top_k": request.top_k or 40,
            "repeat_penalty": request.repeat_penalty or 1.05,  # –£–º–µ–Ω—å—à–∞–µ–º –¥–ª—è –º–µ–Ω—å—à–∏—Ö –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
            "seed": request.seed or -1,
            "stream": True,
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è Llama 3.1 - stop —Ç–æ–∫–µ–Ω—ã
            "stop": ["<|eot_id|>", "<|end_of_text|>", "<|im_end|>", "</s>", "–í–æ–ø—Ä–æ—Å:", "useranswer", "–û—Ç–≤–µ—Ç:"],
        }
        
        logger.debug("ü§ñ LLM streaming –ø–∞—Ä–∞–º–µ—Ç—Ä—ã", 
                    prompt_len=len(formatted_prompt),
                    max_tokens=params["max_tokens"],
                    temperature=params["temperature"],
                    top_p=params["top_p"],
                    top_k=params["top_k"],
                    repeat_penalty=params["repeat_penalty"],
                    seed=params["seed"])
        
        logger.debug("üî§ Streaming –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM", prompt_preview=formatted_prompt[:500])

        def create_stream():
            return self._llama.create_completion(**params)

        async def process_stream():
            stream = create_stream()
            total_content = ""
            chunk_count = 0
            
            logger.debug("üåÄ –ù–∞—á–∞–ª–æ streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
            
            for chunk in stream:
                chunk_count += 1
                logger.debug(f"üì¶ Chunk #{chunk_count}", chunk_keys=list(chunk.keys()) if chunk else None)
                
                if chunk and "choices" in chunk and chunk["choices"]:
                    choice = chunk["choices"][0]
                    logger.debug(f"üîç Choice structure", choice_keys=list(choice.keys()), choice_preview=choice)
                    
                    # llama-cpp –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –≤ "text", –∞ –Ω–µ –≤ "delta"
                    content = None
                    if "delta" in choice and "content" in choice["delta"]:
                        content = choice["delta"]["content"]
                    elif "text" in choice:
                        content = choice["text"]
                    
                    if content:
                        total_content += content
                        logger.debug(f"üí¨ Chunk –∫–æ–Ω—Ç–µ–Ω—Ç", content=repr(content))
                        yield {
                            "id": f"chatcmpl-{uuid.uuid4()}",
                            "object": "chat.completion.chunk",
                            "created": int(time.time()),
                            "model": request.model,
                            "choices": [
                                {
                                    "index": 0,
                                    "delta": {"content": content},
                                    "finish_reason": None,
                                }
                            ],
                        }
                    
                    if choice.get("finish_reason"):
                        logger.debug("üèÅ Streaming –∑–∞–≤–µ—Ä—à–µ–Ω", 
                                   finish_reason=choice["finish_reason"],
                                   total_chunks=chunk_count,
                                   total_content_len=len(total_content))
                        yield {
                            "id": f"chatcmpl-{uuid.uuid4()}",
                            "object": "chat.completion.chunk",
                            "created": int(time.time()),
                            "model": request.model,
                            "choices": [
                                {
                                    "index": 0,
                                    "delta": {},
                                    "finish_reason": choice["finish_reason"],
                                }
                            ],
                        }

        async for chunk in process_stream():
            yield chunk

    async def _process_text_completion(self, request: TextCompletionRequest) -> CompletionResponse:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ text completion –∑–∞–ø—Ä–æ—Å–∞."""
        logger.debug("–û—Ç–ø—Ä–∞–≤–∫–∞ text completion –≤ LLM", prompt_len=len(request.prompt))

        response = self._llama.create_completion(
            prompt=request.prompt,
            max_tokens=request.max_tokens or 1024,
            temperature=request.temperature or 0.7,
            top_p=request.top_p or 0.9,
            top_k=request.top_k or 40,
            repeat_penalty=request.repeat_penalty or 1.1,
            seed=request.seed or -1,
            stream=False,
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è Llama 3.1 - stop —Ç–æ–∫–µ–Ω—ã
            stop=["<|eot_id|>", "<|end_of_text|>", "<|im_end|>", "</s>"],
        )

        return self._create_completion_response(response, request.model, "text_completion")

    async def _process_text_completion_stream(self, request: TextCompletionRequest) -> AsyncIterator[dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ streaming text completion –∑–∞–ø—Ä–æ—Å–∞."""
        logger.debug("–û—Ç–ø—Ä–∞–≤–∫–∞ streaming text completion –≤ LLM", prompt_len=len(request.prompt))

        def create_stream():
            return self._llama.create_completion(
                prompt=request.prompt,
                max_tokens=request.max_tokens or 1024,
                temperature=request.temperature or 0.7,
                top_p=request.top_p or 0.9,
                top_k=request.top_k or 40,
                repeat_penalty=request.repeat_penalty or 1.1,
                seed=request.seed or -1,
                stream=True,
                # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è Llama 3.1 - stop —Ç–æ–∫–µ–Ω—ã
                stop=["<|eot_id|>", "<|end_of_text|>", "<|im_end|>", "</s>"],
            )

        async def process_stream():
            stream = create_stream()
            
            for chunk in stream:
                if chunk and "choices" in chunk and chunk["choices"]:
                    choice = chunk["choices"][0]
                    if "text" in choice:
                        text = choice["text"]
                        if text:
                            yield {
                                "id": f"cmpl-{uuid.uuid4()}",
                                "object": "text_completion",
                                "created": int(time.time()),
                                "model": request.model,
                                "choices": [
                                    {
                                        "index": 0,
                                        "text": text,
                                        "finish_reason": None,
                                    }
                                ],
                            }
                    
                    if choice.get("finish_reason"):
                        yield {
                            "id": f"cmpl-{uuid.uuid4()}",
                            "object": "text_completion",
                            "created": int(time.time()),
                            "model": request.model,
                            "choices": [
                                {
                                    "index": 0,
                                    "text": "",
                                    "finish_reason": choice["finish_reason"],
                                }
                            ],
                        }

        async for chunk in process_stream():
            yield chunk

    async def is_ready(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–∞."""
        return self._llama is not None and self._rag_service is not None

    def _format_chat_messages(self, messages: list[ChatMessage]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è llama.cpp."""
        # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ user —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –æ–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π RAG –ø—Ä–æ–º–ø—Ç
        # (—Å–æ–¥–µ—Ä–∂–∏—Ç Llama 3.1 —Ç–æ–∫–µ–Ω—ã), –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∫–∞–∫ –µ—Å—Ç—å
        if (len(messages) == 1 and 
            messages[0].role == "user" and 
            "<|begin_of_text|>" in messages[0].content):
            return messages[0].content
        
        # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        formatted_parts = []
        
        for message in messages:
            role = message.role
            content = message.content
            
            if role == "system":
                formatted_parts.append(f"System: {content}")
            elif role == "user":
                formatted_parts.append(f"User: {content}")
            elif role == "assistant":
                formatted_parts.append(f"Assistant: {content}")
        
        formatted_parts.append("Assistant:")
        return "\n\n".join(formatted_parts)

    def _create_completion_response(
        self, response: dict, model: str, object_type: str
    ) -> CompletionResponse:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ completion."""
        if not response or "choices" not in response:
            return self._create_error_response("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏")

        choice_data = response["choices"][0]
        
        logger.debug("üîç Raw choice data", choice_keys=list(choice_data.keys()), choice_data=choice_data)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
        if object_type == "chat.completion":
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content = (
                choice_data.get("message", {}).get("content", "") or
                choice_data.get("text", "") or
                ""
            )
        else:
            content = choice_data.get("text", "")
            
        logger.debug("üìù –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç", content_len=len(content), content_preview=content[:100])

        choice = Choice(
            index=0,
            message=ChatMessage(role="assistant", content=content),
            finish_reason=choice_data.get("finish_reason", "stop"),
        )

        usage = Usage(
            prompt_tokens=response.get("usage", {}).get("prompt_tokens", 0),
            completion_tokens=response.get("usage", {}).get("completion_tokens", 0),
            total_tokens=response.get("usage", {}).get("total_tokens", 0),
        )

        return CompletionResponse(
            id=f"chatcmpl-{uuid.uuid4()}",
            object=object_type,
            created=int(time.time()),
            model=model,
            choices=[choice],
            usage=usage,
        )

    def _create_error_response(self, error_message: str) -> CompletionResponse:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ —Å –æ—à–∏–±–∫–æ–π."""
        choice = Choice(
            index=0,
            message=ChatMessage(role="assistant", content=f"–û—à–∏–±–∫–∞: {error_message}"),
            finish_reason="stop",
        )

        return CompletionResponse(
            id=f"chatcmpl-{uuid.uuid4()}",
            object="chat.completion",
            created=int(time.time()),
            model="llama-cpp",
            choices=[choice],
            usage=Usage(prompt_tokens=0, completion_tokens=0, total_tokens=0),
        )

    def _create_stream_error(self, error_message: str) -> dict:
        """–°–æ–∑–¥–∞–Ω–∏–µ streaming –æ—à–∏–±–∫–∏."""
        return {
            "id": f"chatcmpl-{uuid.uuid4()}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": "llama-cpp",
            "choices": [
                {
                    "index": 0,
                    "delta": {"content": f"–û—à–∏–±–∫–∞: {error_message}"},
                    "finish_reason": "stop",
                }
            ],
        }

    def _is_title_generation_request(self, query: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∑–∞–≥–æ–ª–æ–≤–∫–∞."""
        title_keywords = [
            "generate a concise",
            "word title",
            "summarizing the chat",
            "task:",
            "generate title",
            "create title",
            "make title",
            "title for",
            "chat history",
            "conversation title"
        ]
        
        query_lower = query.lower()
        return any(keyword in query_lower for keyword in title_keywords)

    async def _process_simple_completion(self, request: ChatCompletionRequest) -> CompletionResponse:
        """–ü—Ä–æ—Å—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ RAG –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤."""
        try:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            simple_messages = [
                ChatMessage(role="system", content="–¢—ã –ø–æ–º–æ–≥–∞–µ—à—å —Å–æ–∑–¥–∞–≤–∞—Ç—å –∫—Ä–∞—Ç–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è —á–∞—Ç–æ–≤. –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ."),
                *request.messages
            ]
            
            simple_request = ChatCompletionRequest(
                messages=simple_messages,
                model=request.model,
                max_tokens=min(50, request.max_tokens or 50),  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                temperature=0.3,  # –ú–µ–Ω–µ–µ —Ç–≤–æ—Ä—á–µ—Å–∫–∏–π –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                top_p=request.top_p,
                top_k=request.top_k,
                repeat_penalty=request.repeat_penalty,
                seed=request.seed,
                stream=False
            )
            
            return await self._process_completion(simple_request)
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ—Å—Ç–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏", error=str(e))
            return self._create_error_response(f"–û—à–∏–±–∫–∞: {str(e)}") 