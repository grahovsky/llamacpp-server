"""–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLama.cpp –≤ RAG-only —Ä–µ–∂–∏–º–µ."""

import time
import uuid
from collections.abc import AsyncIterator

import structlog
from llama_cpp import Llama

from ..config.settings import get_settings
from ..domain.models import (
    ChatCompletionRequest,
    ChatMessage,
    Choice,
    CompletionResponse,
    TextCompletionRequest,
    Usage,
)
from ..prompts.protocols import PromptServiceProtocol
from ..retrieval.protocols import RAGServiceProtocol
from .history_manager import ChatHistoryManager

logger = structlog.get_logger(__name__)


class LlamaService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLama –º–æ–¥–µ–ª—å—é –≤ RAG-only —Ä–µ–∂–∏–º–µ."""

    def __init__(
        self,
        llama: Llama,
        rag_service: RAGServiceProtocol = None,
        prompt_service: PromptServiceProtocol = None,
        settings = None
    ) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞."""
        self._llama = llama
        self._rag_service = rag_service
        self._prompt_service = prompt_service

        # –ï—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã, –ø–æ–ª—É—á–∞–µ–º –∏—Ö (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        if settings is None:
            settings = get_settings()
        self._settings = settings

        if not rag_service:
            raise ValueError("RAG —Å–µ—Ä–≤–∏—Å –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è RAG-only —Å–∏—Å—Ç–µ–º—ã")

        if not prompt_service:
            raise ValueError("Prompt —Å–µ—Ä–≤–∏—Å –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω")

        self.history_manager = ChatHistoryManager(
            llama_model=llama,
            max_tokens=self._settings.max_history_tokens,
            reserve_tokens=self._settings.max_response_tokens
        )

        logger.info("LlamaService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ RAG-only —Ä–µ–∂–∏–º–µ")

    async def chat_completion(self, request: ChatCompletionRequest) -> CompletionResponse:
        """Chat completion —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º RAG –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤."""
        request_id = str(uuid.uuid4())[:8]  # –ö–æ—Ä–æ—Ç–∫–∏–π ID –¥–ª—è –ª–æ–≥–æ–≤
        
        logger.info("üß† Chat completion —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º RAG", 
                   model=request.model, request_id=request_id)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        user_query = self._extract_user_query(request.messages)

        if not user_query:
            return self._create_error_response("–ù–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞")

        logger.info("üìù –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞", 
                   query_preview=user_query[:100], request_id=request_id)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ RAG —Å–µ—Ä–≤–∏—Å –¥–æ—Å—Ç—É–ø–µ–Ω
        if not self._rag_service:
            logger.error("‚ùå RAG —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —ç—Ç–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞")
            return self._create_error_response("RAG —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if self._is_service_request(user_query):
            logger.info("üõ†Ô∏è –°–ª—É–∂–µ–±–Ω—ã–π –∑–∞–ø—Ä–æ—Å - –ø—Ä—è–º–æ–π proxy –≤ LLM (–±–µ–∑ RAG)", request_id=request_id)
            return await self._process_service_proxy_completion(request, user_query, request_id)
        else:
            logger.info("üß† –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π RAG", request_id=request_id)
            return await self._process_full_rag_completion(request, user_query, request_id)

    async def chat_completion_stream(
        self, request: ChatCompletionRequest
    ) -> AsyncIterator[dict]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥ chat completion —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º RAG –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤."""
        logger.info("üß† Streaming chat completion —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º RAG", model=request.model)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å
        user_query = self._extract_user_query(request.messages)

        if not user_query:
            logger.warning("–ù–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞")
            yield self._create_stream_error("–ù–µ –Ω–∞–π–¥–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å")
            return

        logger.info("üìù –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∏–º–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞", query_preview=user_query[:100])

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ RAG —Å–µ—Ä–≤–∏—Å –¥–æ—Å—Ç—É–ø–µ–Ω
        if not self._rag_service:
            logger.error("‚ùå RAG —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —ç—Ç–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞")
            yield self._create_stream_error("RAG —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if self._is_service_request(user_query):
            logger.info("üõ†Ô∏è –°–ª—É–∂–µ–±–Ω—ã–π —Å—Ç—Ä–∏–º–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å - –ø—Ä—è–º–æ–π proxy –≤ LLM (–±–µ–∑ RAG)")
            async for chunk in self._process_service_proxy_completion_stream(request, user_query):
                yield chunk
        else:
            logger.info("üß† –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Å—Ç—Ä–∏–º–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π RAG")
            async for chunk in self._process_full_rag_completion_stream(request, user_query):
                yield chunk

    async def text_completion(self, request: TextCompletionRequest) -> CompletionResponse:
        """Text completion —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º RAG."""
        logger.info("üß† RAG-only text completion", model=request.model)

        if not request.prompt:
            return self._create_error_response("–ü—É—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç")

        try:
            # –°–æ–∑–¥–∞–µ–º RAG –ø—Ä–æ–º–ø—Ç
            rag_prompt = await self._rag_service.create_rag_prompt(request.prompt)

            # –°–æ–∑–¥–∞–µ–º RAG –∑–∞–ø—Ä–æ—Å
            rag_request = TextCompletionRequest(
                prompt=rag_prompt,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k,
                repeat_penalty=request.repeat_penalty,
                seed=request.seed,
                stream=request.stream
            )

            logger.info("‚úÖ RAG text completion –ø—Ä–æ–º–ø—Ç —Å–æ–∑–¥–∞–Ω")

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
            return await self._process_text_completion(rag_request)

        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ RAG text completion", error=str(e), exc_info=True)
            return self._create_error_response(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

    async def text_completion_stream(
        self, request: TextCompletionRequest
    ) -> AsyncIterator[dict]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥ text completion —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º RAG."""
        logger.info("üß† RAG-only streaming text completion", model=request.model)

        if not request.prompt:
            yield self._create_stream_error("–ü—É—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç")
            return

        try:
            # –°–æ–∑–¥–∞–µ–º RAG –ø—Ä–æ–º–ø—Ç
            rag_prompt = await self._rag_service.create_rag_prompt(request.prompt)

            # –°–æ–∑–¥–∞–µ–º RAG –∑–∞–ø—Ä–æ—Å
            rag_request = TextCompletionRequest(
                prompt=rag_prompt,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k,
                repeat_penalty=request.repeat_penalty,
                seed=request.seed,
                stream=True
            )

            logger.info("‚úÖ RAG streaming text completion –ø—Ä–æ–º–ø—Ç —Å–æ–∑–¥–∞–Ω")

            # –°—Ç—Ä–∏–º–∏–º –æ—Ç–≤–µ—Ç
            async for chunk in self._process_text_completion_stream(rag_request):
                yield chunk

        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ RAG text completion —Å—Ç—Ä–∏–º–∏–Ω–≥–∞", error=str(e), exc_info=True)
            yield self._create_stream_error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

    def _extract_user_query(self, messages: list[ChatMessage]) -> str:
        """–ò–∑–≤–ª–µ—á—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å."""
        for msg in reversed(messages):
            if msg.role == "user" and msg.content.strip():
                return msg.content.strip()
        return ""

    async def _process_completion(self, request: ChatCompletionRequest) -> CompletionResponse:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ completion –∑–∞–ø—Ä–æ—Å–∞."""
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–ª—è llama.cpp
        formatted_prompt = self._format_chat_messages(request.messages)

        # üîç –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ì–û –ü–†–û–ú–ü–¢–ê
        logger.info("üî§ –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM", 
                   prompt_length=len(formatted_prompt),
                   starts_with_begin_of_text=formatted_prompt.startswith("<|begin_of_text|>"))
        
        # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        logger.info("üìã –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–ø–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤):", 
                   final_prompt_preview=formatted_prompt[:1000])

        # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ RAG –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π max_tokens
        llm_params = self._settings.build_rag_params(
            is_title=False,
            max_tokens=request.max_tokens  # –ü–µ—Ä–µ–¥–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ª–∏–º–∏—Ç –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        )

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        params = {
            "prompt": formatted_prompt,
            "stream": False,
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è Llama 3.1 - stop —Ç–æ–∫–µ–Ω—ã
            "stop": ["<|eot_id|>", "<|end_of_text|>", "<|im_end|>", "</s>", "–í–æ–ø—Ä–æ—Å:", "useranswer", "–û—Ç–≤–µ—Ç:", "### Task:", "### Guidelines:", "### Output:", "### Examples:", "### Chat History:", "User:", "System:", "Assistant:"],
            **llm_params
        }

        logger.debug("ü§ñ LLM –ø–∞—Ä–∞–º–µ—Ç—Ä—ã",
                    prompt_len=len(formatted_prompt),
                    **{k: v for k, v in params.items() if k != "prompt" and k != "stop"})

        logger.debug("üî§ –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM", prompt_preview=formatted_prompt[:500])

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = self._llama.create_completion(**params)

        logger.debug("üéØ LLM –æ—Ç–≤–µ—Ç", response_keys=list(response.keys()) if response else None)
        if response and "choices" in response:
            choice = response["choices"][0]
            content = choice.get("message", {}).get("content", choice.get("text", ""))
            logger.debug("üìù –ö–æ–Ω—Ç–µ–Ω—Ç –æ—Ç–≤–µ—Ç–∞",
                        content_len=len(content),
                        content_preview=content[:200],
                        finish_reason=choice.get("finish_reason"))

        return self._create_completion_response(response, request.model, "chat.completion")

    async def _process_completion_stream(self, request: ChatCompletionRequest) -> AsyncIterator[dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ streaming completion –∑–∞–ø—Ä–æ—Å–∞."""
        formatted_prompt = self._format_chat_messages(request.messages)

        # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ RAG –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π max_tokens
        llm_params = self._settings.build_rag_params(
            is_title=False,
            max_tokens=request.max_tokens  # –ü–µ—Ä–µ–¥–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ª–∏–º–∏—Ç –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        )

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        params = {
            "prompt": formatted_prompt,
            "stream": True,
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è Llama 3.1 - stop —Ç–æ–∫–µ–Ω—ã
            "stop": ["<|eot_id|>", "<|end_of_text|>", "<|im_end|>", "</s>", "–í–æ–ø—Ä–æ—Å:", "useranswer", "–û—Ç–≤–µ—Ç:", "### Task:", "### Guidelines:", "### Output:", "### Examples:", "### Chat History:", "User:", "System:", "Assistant:"],
            **llm_params
        }

        logger.debug("ü§ñ LLM streaming –ø–∞—Ä–∞–º–µ—Ç—Ä—ã",
                    prompt_len=len(formatted_prompt),
                    **{k: v for k, v in params.items() if k != "prompt" and k != "stop"})

        logger.debug("üî§ Streaming –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM", prompt_preview=formatted_prompt[:500])

        # –ó–∞–ø—É—Å–∫–∞–µ–º streaming —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, –Ω–æ –¥–µ–ª–∞–µ–º –µ–≥–æ async —á–µ—Ä–µ–∑ yield
        logger.info("üåä –°–æ–∑–¥–∞–Ω–∏–µ streaming –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞")
        stream = self._llama.create_completion(**params)
        total_content = ""
        chunk_count = 0

        # –ü—Ä–æ—Å—Ç–∞—è –∑–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞
        max_tokens = self._settings.get_effective_max_tokens(request.max_tokens)
        token_count = 0

        logger.info("üåÄ –ù–∞—á–∞–ª–æ streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", max_tokens=max_tokens)

        try:
            for chunk in stream:
                chunk_count += 1
                logger.debug(f"üì¶ Chunk #{chunk_count}", chunk_keys=list(chunk.keys()) if chunk else None)

                if chunk and "choices" in chunk and chunk["choices"]:
                    choice = chunk["choices"][0]
                    logger.debug("üîç Choice structure", choice_keys=list(choice.keys()), choice_preview=choice)

                    # llama-cpp –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –≤ "text", –∞ –Ω–µ –≤ "delta"
                    content = None
                    if "delta" in choice and "content" in choice["delta"]:
                        content = choice["delta"]["content"]
                    elif "text" in choice:
                        content = choice["text"]

                    if content:
                        total_content += content
                        token_count += 1

                        # –ü—Ä–æ—Å—Ç–∞—è –∑–∞—â–∏—Ç–∞ –æ—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
                        if token_count >= max_tokens:
                            logger.warning("üõë –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤",
                                         max_tokens=max_tokens,
                                         total_chunks=chunk_count,
                                         total_content_len=len(total_content))
                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π chunk —Å –ø—Ä–∏—á–∏–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                            yield {
                                "id": f"chatcmpl-{uuid.uuid4()}",
                                "object": "chat.completion.chunk",
                                "created": int(time.time()),
                                "model": request.model,
                                "choices": [
                                    {
                                        "index": 0,
                                        "delta": {},
                                        "finish_reason": "length",
                                    }
                                ],
                            }
                            break

                        logger.debug("üí¨ Chunk –∫–æ–Ω—Ç–µ–Ω—Ç", content=repr(content))
                        yield {
                            "id": f"chatcmpl-{uuid.uuid4()}",
                            "object": "chat.completion.chunk",
                            "created": int(time.time()),
                            "model": request.model,
                            "choices": [
                                {
                                    "index": 0,
                                    "delta": {"content": content},
                                    "finish_reason": None,
                                }
                            ],
                        }

                    if choice.get("finish_reason"):
                        logger.info("üèÅ Streaming –∑–∞–≤–µ—Ä—à–µ–Ω",
                                   finish_reason=choice["finish_reason"],
                                   total_chunks=chunk_count,
                                   total_content_len=len(total_content))
                        yield {
                            "id": f"chatcmpl-{uuid.uuid4()}",
                            "object": "chat.completion.chunk",
                            "created": int(time.time()),
                            "model": request.model,
                            "choices": [
                                {
                                    "index": 0,
                                    "delta": {},
                                    "finish_reason": choice["finish_reason"],
                                }
                            ],
                        }
                        break

        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –≤ streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", error=str(e), exc_info=True)
            yield self._create_stream_error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}")

    async def _process_text_completion(self, request: TextCompletionRequest) -> CompletionResponse:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ text completion –∑–∞–ø—Ä–æ—Å–∞."""
        logger.debug("–û—Ç–ø—Ä–∞–≤–∫–∞ text completion –≤ LLM", prompt_len=len(request.prompt))

        # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ RAG –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (text completion —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º)
        llm_params = self._settings.merge_request_params(request, is_title=False)

        response = self._llama.create_completion(
            prompt=request.prompt,
            stream=False,
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è Llama 3.1 - stop —Ç–æ–∫–µ–Ω—ã
            stop=["<|eot_id|>", "<|end_of_text|>", "<|im_end|>", "</s>"],
            **llm_params
        )

        return self._create_completion_response(response, request.model, "text_completion")

    async def _process_text_completion_stream(self, request: TextCompletionRequest) -> AsyncIterator[dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ streaming text completion –∑–∞–ø—Ä–æ—Å–∞."""
        logger.debug("–û—Ç–ø—Ä–∞–≤–∫–∞ streaming text completion –≤ LLM", prompt_len=len(request.prompt))

        # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ RAG –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (text completion —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º)
        llm_params = self._settings.merge_request_params(request, is_title=False)

        # –ó–∞–ø—É—Å–∫–∞–µ–º streaming —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, –Ω–æ –¥–µ–ª–∞–µ–º –µ–≥–æ async —á–µ—Ä–µ–∑ yield
        stream = self._llama.create_completion(
            prompt=request.prompt,
            stream=True,
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è Llama 3.1 - stop —Ç–æ–∫–µ–Ω—ã
            stop=["<|eot_id|>", "<|end_of_text|>", "<|im_end|>", "</s>"],
            **llm_params
        )

        try:
            for chunk in stream:
                if chunk and "choices" in chunk and chunk["choices"]:
                    choice = chunk["choices"][0]
                    if "text" in choice:
                        text = choice["text"]
                        if text:
                            yield {
                                "id": f"cmpl-{uuid.uuid4()}",
                                "object": "text_completion",
                                "created": int(time.time()),
                                "model": request.model,
                                "choices": [
                                    {
                                        "index": 0,
                                        "text": text,
                                        "finish_reason": None,
                                    }
                                ],
                            }

                    if choice.get("finish_reason"):
                        yield {
                            "id": f"cmpl-{uuid.uuid4()}",
                            "object": "text_completion",
                            "created": int(time.time()),
                            "model": request.model,
                            "choices": [
                                {
                                    "index": 0,
                                    "text": "",
                                    "finish_reason": choice["finish_reason"],
                                }
                            ],
                        }
                        break

        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –≤ text completion streaming", error=str(e), exc_info=True)
            yield self._create_stream_error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}")

    async def is_ready(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–∞."""
        return self._llama is not None and self._rag_service is not None

    def _format_chat_messages(self, messages: list[ChatMessage]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è llama.cpp."""
        # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ user —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –æ–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π RAG –ø—Ä–æ–º–ø—Ç
        # (—Å–æ–¥–µ—Ä–∂–∏—Ç Llama 3.1 —Ç–æ–∫–µ–Ω—ã), –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∫–∞–∫ –µ—Å—Ç—å
        if (len(messages) == 1 and
            messages[0].role == "user" and
            "<|begin_of_text|>" in messages[0].content):
            
            # –í–ê–ñ–ù–û: —É–±–∏—Ä–∞–µ–º <|begin_of_text|> —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
            # llama.cpp —Å–∞–º –¥–æ–±–∞–≤–∏—Ç —ç—Ç–æ—Ç —Ç–æ–∫–µ–Ω
            content = messages[0].content
            if content.startswith("<|begin_of_text|>"):
                content = content[len("<|begin_of_text|>"):]
                logger.debug("üîß –£–¥–∞–ª–µ–Ω –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π <|begin_of_text|> —Ç–æ–∫–µ–Ω")
            
            return content

        # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        formatted_parts = []

        for message in messages:
            role = message.role
            content = message.content

            if role == "system":
                formatted_parts.append(f"System: {content}")
            elif role == "user":
                formatted_parts.append(f"User: {content}")
            elif role == "assistant":
                formatted_parts.append(f"Assistant: {content}")

        formatted_parts.append("Assistant:")
        return "\n\n".join(formatted_parts)

    def _create_completion_response(
        self, response: dict, model: str, object_type: str
    ) -> CompletionResponse:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ completion."""
        if not response or "choices" not in response:
            logger.warning("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏")
            return self._create_error_response("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏")

        choice_data = response["choices"][0]

        logger.debug("üîç Raw choice data", choice_keys=list(choice_data.keys()), choice_data=choice_data)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
        if object_type == "chat.completion":
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content = (
                choice_data.get("message", {}).get("content", "") or
                choice_data.get("text", "") or
                ""
            )
        else:
            content = choice_data.get("text", "")

        logger.debug("üìù –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç", content_len=len(content), content_preview=content[:100])

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –ø—É—Å—Ç–æ–π
        if not content or not content.strip():
            logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
            content = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —Å–º–æ–≥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å."

        choice = Choice(
            index=0,
            message=ChatMessage(role="assistant", content=content),
            finish_reason=choice_data.get("finish_reason", "stop"),
        )

        usage = Usage(
            prompt_tokens=response.get("usage", {}).get("prompt_tokens", 0),
            completion_tokens=response.get("usage", {}).get("completion_tokens", 0),
            total_tokens=response.get("usage", {}).get("total_tokens", 0),
        )

        return CompletionResponse(
            id=f"chatcmpl-{uuid.uuid4()}",
            object=object_type,
            created=int(time.time()),
            model=model,
            choices=[choice],
            usage=usage,
        )

    def _create_error_response(self, error_message: str) -> CompletionResponse:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ —Å –æ—à–∏–±–∫–æ–π."""
        choice = Choice(
            index=0,
            message=ChatMessage(role="assistant", content=f"–û—à–∏–±–∫–∞: {error_message}"),
            finish_reason="stop",
        )

        return CompletionResponse(
            id=f"chatcmpl-{uuid.uuid4()}",
            object="chat.completion",
            created=int(time.time()),
            model="llama-cpp",
            choices=[choice],
            usage=Usage(prompt_tokens=0, completion_tokens=0, total_tokens=0),
        )

    def _create_stream_error(self, error_message: str) -> dict:
        """–°–æ–∑–¥–∞–Ω–∏–µ streaming –æ—à–∏–±–∫–∏."""
        return {
            "id": f"chatcmpl-{uuid.uuid4()}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": "llama-cpp",
            "choices": [
                {
                    "index": 0,
                    "delta": {"content": f"–û—à–∏–±–∫–∞: {error_message}"},
                    "finish_reason": "stop",
                }
            ],
        }

    def _is_service_request(self, query: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å —Å–ª—É–∂–µ–±–Ω—ã–º (–∑–∞–≥–æ–ª–æ–≤–∫–∏, —Ç–µ–≥–∏, etc)."""
        query_lower = query.lower()

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Å–ª—É–∂–µ–±–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        service_keywords = [
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            "generate a concise",
            "word title",
            "summarizing the chat",
            "generate title",
            "create title",
            "make title",
            "title for",
            "title with an emoji",
            "conversation title",

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–≥–æ–≤
            "generate 1-3 broad tags",
            "categorizing the main themes",
            "tags categorizing",
            "broad tags",
            "specific subtopic tags",
            "themes of the chat",

            # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ Open WebUI
            "### task:",
            "### guidelines:",
            "### output:",
            "### examples:",
            "### chat history:",
            "<chat_history>",
            "</chat_history>",

            # –§–æ—Ä–º–∞—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤
            "json format:",
            "output format:",
            "response format:",
            "format: {",
            '"title":',
            '"tags":',

            # –ê–Ω–∞–ª–∏–∑ —á–∞—Ç–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            "analyze the conversation",
            "summarize this chat",
            "extract key topics",
            "main subjects discussed",
            "primary language",
            "multilingual",

            # –°–∏—Å—Ç–µ–º–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            "your entire response must consist",
            "without any introductory",
            "raw json object",
            "without markdown code fences",
            "parsing failure",
            "single, raw json",
            "ensure no conversational text",
        ]

        is_service = any(keyword in query_lower for keyword in service_keywords)

        if is_service:
            logger.info("üîç –°–ª—É–∂–µ–±–Ω—ã–π –∑–∞–ø—Ä–æ—Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω", 
                       keywords=[kw for kw in service_keywords if kw in query_lower])

        return is_service

    def _is_title_generation_request(self, query: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∑–∞–≥–æ–ª–æ–≤–∫–∞."""
        # –¢–µ–ø–µ—Ä—å —ç—Ç–æ —á–∞—Å—Ç–Ω—ã–π —Å–ª—É—á–∞–π —Å–ª—É–∂–µ–±–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        title_keywords = [
            "generate a concise",
            "word title",
            "summarizing the chat",
            "task:",
            "generate title",
            "create title",
            "make title",
            "title for",
            "chat history",
            "conversation title",
            "title with an emoji"
        ]

        query_lower = query.lower()
        return any(keyword in query_lower for keyword in title_keywords)

    async def _process_service_proxy_completion(self, request: ChatCompletionRequest, user_query: str, request_id: str) -> CompletionResponse:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É–∂–µ–±–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∫–∞–∫ –ø—Ä—è–º–æ–≥–æ proxy –≤ LLM."""
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–ª—É–∂–µ–±–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            is_title = self._is_title_generation_request(user_query)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–π –∑–∞–ø—Ä–æ—Å –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ Llama 3.1
            if is_title:
                # –î–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
                formatted_prompt = (
                    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n"
                    "Generate a concise JSON title response.\n"
                    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n"
                    f"{user_query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
                )
                max_tokens = 50
                temperature = 0.1
            else:
                # –î–ª—è –¥—Ä—É–≥–∏—Ö —Å–ª—É–∂–µ–±–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
                formatted_prompt = (
                    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n"
                    "You are a helpful assistant. Provide concise, accurate responses.\n"
                    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n"
                    f"{user_query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
                )
                max_tokens = 200
                temperature = 0.3
            
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö —Å–ª—É–∂–µ–±–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
            response = self._llama.create_completion(
                prompt=formatted_prompt,
                stream=False,
                max_tokens=max_tokens,
                temperature=temperature,
                top_k=20,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                top_p=0.8,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                repeat_penalty=1.0,  # –ë–µ–∑ —à—Ç—Ä–∞—Ñ–æ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è Llama 3.1 - stop —Ç–æ–∫–µ–Ω—ã
                stop=["<|eot_id|>", "<|end_of_text|>", "<|im_end|>", "</s>"],
            )

            logger.info("‚úÖ –°–ª—É–∂–µ–±–Ω—ã–π –∑–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∫–∞–∫ –ø—Ä—è–º–æ–π proxy –≤ LLM", 
                       is_title=is_title, max_tokens=max_tokens, temp=temperature, request_id=request_id)

            return self._create_completion_response(response, request.model, "chat.completion")

        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª—É–∂–µ–±–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞", error=str(e), exc_info=True, request_id=request_id)
            return self._create_error_response(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

    async def _process_service_proxy_completion_stream(self, request: ChatCompletionRequest, user_query: str) -> AsyncIterator[dict]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É–∂–µ–±–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∫–∞–∫ –ø—Ä—è–º–æ–≥–æ proxy –≤ LLM."""
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–ª—É–∂–µ–±–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            is_title = self._is_title_generation_request(user_query)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–π –∑–∞–ø—Ä–æ—Å –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ Llama 3.1
            if is_title:
                # –î–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
                formatted_prompt = (
                    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n"
                    "Generate a concise JSON title response.\n"
                    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n"
                    f"{user_query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
                )
                max_tokens = 50
                temperature = 0.1
            else:
                # –î–ª—è –¥—Ä—É–≥–∏—Ö —Å–ª—É–∂–µ–±–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
                formatted_prompt = (
                    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n"
                    "You are a helpful assistant. Provide concise, accurate responses.\n"
                    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n"
                    f"{user_query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
                )
                max_tokens = 200
                temperature = 0.3
            
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö —Å–ª—É–∂–µ–±–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
            stream = self._llama.create_completion(
                prompt=formatted_prompt,
                stream=True,
                max_tokens=max_tokens,
                temperature=temperature,
                top_k=20,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                top_p=0.8,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                repeat_penalty=1.0,  # –ë–µ–∑ —à—Ç—Ä–∞—Ñ–æ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è Llama 3.1 - stop —Ç–æ–∫–µ–Ω—ã
                stop=["<|eot_id|>", "<|end_of_text|>", "<|im_end|>", "</s>"],
            )

            logger.info("‚úÖ –°–ª—É–∂–µ–±–Ω—ã–π –∑–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∫–∞–∫ –ø—Ä—è–º–æ–π proxy –≤ LLM (streaming)", 
                       is_title=is_title, max_tokens=max_tokens, temp=temperature)

            # –°—Ç—Ä–∏–º–∏–º –æ—Ç–≤–µ—Ç –Ω–∞–ø—Ä—è–º—É—é –æ—Ç llama
            chunk_count = 0
            for chunk in stream:
                chunk_count += 1
                logger.debug(f"üîÑ –°–ª—É–∂–µ–±–Ω—ã–π chunk #{chunk_count}")
                
                yield {
                    "choices": [
                        {
                            "delta": {
                                "content": chunk.get("choices", [{}])[0].get("text", "")
                            },
                            "finish_reason": chunk.get("choices", [{}])[0].get("finish_reason")
                        }
                    ],
                    "object": "chat.completion.chunk",
                    "model": request.model
                }

        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª—É–∂–µ–±–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞", error=str(e), exc_info=True)
            yield self._create_stream_error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

    async def _process_full_rag_completion(self, request: ChatCompletionRequest, user_query: str, request_id: str) -> CompletionResponse:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–ª–Ω—ã–º RAG."""
        try:
            # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π RAG –ø—Ä–æ–º–ø—Ç —Å –ø–æ–∏—Å–∫–æ–º –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
            rag_prompt = await self._rag_service.create_rag_prompt(user_query)

            # üîç –î–ï–¢–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –î–õ–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò
            logger.info("üîç RAG –ø—Ä–æ–º–ø—Ç –≥–æ—Ç–æ–≤ –∫ –æ—Ç–ø—Ä–∞–≤–∫–µ", 
                       rag_prompt_length=len(rag_prompt),
                       contains_begin_of_text="<|begin_of_text|>" in rag_prompt, request_id=request_id)
            
            # –í—ã–≤–æ–¥–∏–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            logger.debug("üî§ –ü–û–õ–ù–´–ô RAG –ü–†–û–ú–ü–¢:", rag_prompt=rag_prompt)
            
            # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤ –≤ info –ª–æ–≥–µ
            logger.info("üìù RAG –ø—Ä–æ–º–ø—Ç (–ø–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤):", 
                       rag_preview=rag_prompt[:1000])

            # RAG –ø—Ä–æ–º–ø—Ç —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å RAG –ø—Ä–æ–º–ø—Ç–æ–º
            rag_messages = [
                ChatMessage(role="user", content=rag_prompt)
            ]

            # –°–æ–∑–¥–∞–µ–º RAG –∑–∞–ø—Ä–æ—Å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            llm_params = self._settings.merge_request_params(request, is_title=False)

            rag_request = ChatCompletionRequest(
                messages=rag_messages,
                model=request.model,
                stream=False,
                **llm_params
            )

            logger.info("‚úÖ –ü–æ–ª–Ω—ã–π RAG –ø—Ä–æ–º–ø—Ç —Å–æ–∑–¥–∞–Ω",
                       original_len=len(user_query),
                       rag_len=len(rag_prompt), request_id=request_id)

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
            return await self._process_completion(rag_request)

        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–π RAG –æ–±—Ä–∞–±–æ—Ç–∫–∏", error=str(e), exc_info=True, request_id=request_id)
            return self._create_error_response(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

    async def _process_full_rag_completion_stream(self, request: ChatCompletionRequest, user_query: str) -> AsyncIterator[dict]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–ª–Ω—ã–º RAG."""
        try:
            # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π RAG –ø—Ä–æ–º–ø—Ç —Å –ø–æ–∏—Å–∫–æ–º –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
            rag_prompt = await self._rag_service.create_rag_prompt(user_query)

            # üîç –î–ï–¢–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –î–õ–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò
            logger.info("üîç RAG –ø—Ä–æ–º–ø—Ç –≥–æ—Ç–æ–≤ –∫ –æ—Ç–ø—Ä–∞–≤–∫–µ (streaming)", 
                       rag_prompt_length=len(rag_prompt),
                       contains_begin_of_text="<|begin_of_text|>" in rag_prompt)
            
            # –í—ã–≤–æ–¥–∏–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            logger.debug("üî§ –ü–û–õ–ù–´–ô RAG –ü–†–û–ú–ü–¢ (streaming):", rag_prompt=rag_prompt)
            
            # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤ –≤ info –ª–æ–≥–µ
            logger.info("üìù RAG –ø—Ä–æ–º–ø—Ç (–ø–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤, streaming):", 
                       rag_preview=rag_prompt[:1000])

            # RAG –ø—Ä–æ–º–ø—Ç —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å RAG –ø—Ä–æ–º–ø—Ç–æ–º
            rag_messages = [
                ChatMessage(role="user", content=rag_prompt)
            ]

            # –°–æ–∑–¥–∞–µ–º RAG –∑–∞–ø—Ä–æ—Å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            llm_params = self._settings.merge_request_params(request, is_title=False)

            rag_request = ChatCompletionRequest(
                messages=rag_messages,
                model=request.model,
                stream=True,
                **llm_params
            )

            logger.info("‚úÖ –ü–æ–ª–Ω—ã–π RAG —Å—Ç—Ä–∏–º–∏–Ω–≥ –ø—Ä–æ–º–ø—Ç —Å–æ–∑–¥–∞–Ω")

            # –°—Ç—Ä–∏–º–∏–º –æ—Ç–≤–µ—Ç
            async for chunk in self._process_completion_stream(rag_request):
                yield chunk

        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–≥–æ RAG —Å—Ç—Ä–∏–º–∏–Ω–≥–∞", error=str(e), exc_info=True)
            yield self._create_stream_error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")
