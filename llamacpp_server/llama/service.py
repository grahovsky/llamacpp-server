"""–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLama.cpp –≤ RAG-only —Ä–µ–∂–∏–º–µ."""

import time
import uuid
from collections.abc import AsyncIterator

import structlog
from llama_cpp import Llama

from ..config.settings import get_settings
from ..domain.models import (
    ChatCompletionRequest,
    ChatMessage,
    Choice,
    CompletionResponse,
    TextCompletionRequest,
    Usage,
)
from ..retrieval.protocols import RAGServiceProtocol
from ..prompts.templates import SYSTEM_PROMPT
from .history_manager import ChatHistoryManager

logger = structlog.get_logger(__name__)


class LlamaService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLama –º–æ–¥–µ–ª—å—é –≤ RAG-only —Ä–µ–∂–∏–º–µ."""

    def __init__(
        self,
        llama: Llama,
        rag_service: RAGServiceProtocol = None
    ) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞."""
        self._llama = llama
        self._rag_service = rag_service
        settings = get_settings()
        
        if not rag_service:
            raise ValueError("RAG —Å–µ—Ä–≤–∏—Å –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è RAG-only —Å–∏—Å—Ç–µ–º—ã")
        
        self.history_manager = ChatHistoryManager(
            llama_model=llama,
            max_tokens=settings.max_history_tokens,
            reserve_tokens=settings.max_response_tokens
        )
        
        logger.info("LlamaService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ RAG-only —Ä–µ–∂–∏–º–µ")

    async def chat_completion(self, request: ChatCompletionRequest) -> CompletionResponse:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è chat completion —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º RAG."""
        logger.info("üß† RAG-only chat completion", model=request.model)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å
        user_query = self._extract_user_query(request.messages)
        
        if not user_query:
            logger.warning("–ù–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞")
            return self._create_error_response("–ù–µ –Ω–∞–π–¥–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å")

        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞", query_preview=user_query[:100])

        try:
            # –°–æ–∑–¥–∞–µ–º RAG –ø—Ä–æ–º–ø—Ç –¥–ª—è –ª—é–±–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            rag_prompt = await self._rag_service.create_rag_prompt(user_query)
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º –∏ RAG –ø—Ä–æ–º–ø—Ç–æ–º
            rag_messages = [
                ChatMessage(role="system", content=SYSTEM_PROMPT),
                ChatMessage(role="user", content=rag_prompt)
            ]

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å RAG —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
            rag_request = ChatCompletionRequest(
                messages=rag_messages,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k,
                repeat_penalty=request.repeat_penalty,
                seed=request.seed,
                stream=request.stream
            )

            logger.info("‚úÖ RAG –ø—Ä–æ–º–ø—Ç —Å–æ–∑–¥–∞–Ω", 
                       original_len=len(user_query),
                       rag_len=len(rag_prompt))

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
            return await self._process_completion(rag_request)

        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ RAG –æ–±—Ä–∞–±–æ—Ç–∫–∏", error=str(e), exc_info=True)
            return self._create_error_response(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

    async def chat_completion_stream(
        self, request: ChatCompletionRequest
    ) -> AsyncIterator[dict]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥ chat completion —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º RAG."""
        logger.info("üß† RAG-only streaming chat completion", model=request.model)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å
        user_query = self._extract_user_query(request.messages)
        
        if not user_query:
            logger.warning("–ù–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞")
            yield self._create_stream_error("–ù–µ –Ω–∞–π–¥–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å")
            return

        try:
            # –°–æ–∑–¥–∞–µ–º RAG –ø—Ä–æ–º–ø—Ç
            rag_prompt = await self._rag_service.create_rag_prompt(user_query)
            
            # –°–æ–∑–¥–∞–µ–º RAG —Å–æ–æ–±—â–µ–Ω–∏—è
            rag_messages = [
                ChatMessage(role="system", content=SYSTEM_PROMPT),
                ChatMessage(role="user", content=rag_prompt)
            ]

            # –°–æ–∑–¥–∞–µ–º RAG –∑–∞–ø—Ä–æ—Å
            rag_request = ChatCompletionRequest(
                messages=rag_messages,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k,
                repeat_penalty=request.repeat_penalty,
                seed=request.seed,
                stream=True
            )

            logger.info("‚úÖ RAG —Å—Ç—Ä–∏–º–∏–Ω–≥ –ø—Ä–æ–º–ø—Ç —Å–æ–∑–¥–∞–Ω")

            # –°—Ç—Ä–∏–º–∏–º –æ—Ç–≤–µ—Ç
            async for chunk in self._process_completion_stream(rag_request):
                yield chunk

        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ RAG —Å—Ç—Ä–∏–º–∏–Ω–≥–∞", error=str(e), exc_info=True)
            yield self._create_stream_error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

    async def text_completion(self, request: TextCompletionRequest) -> CompletionResponse:
        """Text completion —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º RAG."""
        logger.info("üß† RAG-only text completion", model=request.model)

        if not request.prompt:
            return self._create_error_response("–ü—É—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç")

        try:
            # –°–æ–∑–¥–∞–µ–º RAG –ø—Ä–æ–º–ø—Ç
            rag_prompt = await self._rag_service.create_rag_prompt(request.prompt)
            
            # –°–æ–∑–¥–∞–µ–º RAG –∑–∞–ø—Ä–æ—Å
            rag_request = TextCompletionRequest(
                prompt=rag_prompt,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k,
                repeat_penalty=request.repeat_penalty,
                seed=request.seed,
                stream=request.stream
            )

            logger.info("‚úÖ RAG text completion –ø—Ä–æ–º–ø—Ç —Å–æ–∑–¥–∞–Ω")

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
            return await self._process_text_completion(rag_request)

        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ RAG text completion", error=str(e), exc_info=True)
            return self._create_error_response(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

    async def text_completion_stream(
        self, request: TextCompletionRequest
    ) -> AsyncIterator[dict]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥ text completion —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º RAG."""
        logger.info("üß† RAG-only streaming text completion", model=request.model)

        if not request.prompt:
            yield self._create_stream_error("–ü—É—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç")
            return

        try:
            # –°–æ–∑–¥–∞–µ–º RAG –ø—Ä–æ–º–ø—Ç
            rag_prompt = await self._rag_service.create_rag_prompt(request.prompt)
            
            # –°–æ–∑–¥–∞–µ–º RAG –∑–∞–ø—Ä–æ—Å
            rag_request = TextCompletionRequest(
                prompt=rag_prompt,
                model=request.model,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k,
                repeat_penalty=request.repeat_penalty,
                seed=request.seed,
                stream=True
            )

            logger.info("‚úÖ RAG streaming text completion –ø—Ä–æ–º–ø—Ç —Å–æ–∑–¥–∞–Ω")

            # –°—Ç—Ä–∏–º–∏–º –æ—Ç–≤–µ—Ç
            async for chunk in self._process_text_completion_stream(rag_request):
                yield chunk

        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ RAG text completion —Å—Ç—Ä–∏–º–∏–Ω–≥–∞", error=str(e), exc_info=True)
            yield self._create_stream_error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

    def _extract_user_query(self, messages: list[ChatMessage]) -> str:
        """–ò–∑–≤–ª–µ—á—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å."""
        for msg in reversed(messages):
            if msg.role == "user" and msg.content.strip():
                return msg.content.strip()
        return ""

    async def _process_completion(self, request: ChatCompletionRequest) -> CompletionResponse:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ completion –∑–∞–ø—Ä–æ—Å–∞."""
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–ª—è llama.cpp
        formatted_prompt = self._format_chat_messages(request.messages)
        
        logger.debug("–û—Ç–ø—Ä–∞–≤–∫–∞ –≤ LLM", prompt_len=len(formatted_prompt))

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = self._llama.create_completion(
            prompt=formatted_prompt,
            max_tokens=request.max_tokens or 1024,
            temperature=request.temperature or 0.7,
            top_p=request.top_p or 0.9,
            top_k=request.top_k or 40,
            repeat_penalty=request.repeat_penalty or 1.1,
            seed=request.seed or -1,
            stream=False,
        )

        return self._create_completion_response(response, request.model, "chat.completion")

    async def _process_completion_stream(self, request: ChatCompletionRequest) -> AsyncIterator[dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ streaming completion –∑–∞–ø—Ä–æ—Å–∞."""
        formatted_prompt = self._format_chat_messages(request.messages)
        
        logger.debug("–û—Ç–ø—Ä–∞–≤–∫–∞ streaming –≤ LLM", prompt_len=len(formatted_prompt))

        def create_stream():
            return self._llama.create_completion(
                prompt=formatted_prompt,
                max_tokens=request.max_tokens or 1024,
                temperature=request.temperature or 0.7,
                top_p=request.top_p or 0.9,
                top_k=request.top_k or 40,
                repeat_penalty=request.repeat_penalty or 1.1,
                seed=request.seed or -1,
                stream=True,
            )

        async def process_stream():
            stream = create_stream()
            
            for chunk in stream:
                if chunk and "choices" in chunk and chunk["choices"]:
                    choice = chunk["choices"][0]
                    if "delta" in choice and "content" in choice["delta"]:
                        content = choice["delta"]["content"]
                        if content:
                            yield {
                                "id": f"chatcmpl-{uuid.uuid4()}",
                                "object": "chat.completion.chunk",
                                "created": int(time.time()),
                                "model": request.model,
                                "choices": [
                                    {
                                        "index": 0,
                                        "delta": {"content": content},
                                        "finish_reason": None,
                                    }
                                ],
                            }
                    
                    if choice.get("finish_reason"):
                        yield {
                            "id": f"chatcmpl-{uuid.uuid4()}",
                            "object": "chat.completion.chunk",
                            "created": int(time.time()),
                            "model": request.model,
                            "choices": [
                                {
                                    "index": 0,
                                    "delta": {},
                                    "finish_reason": choice["finish_reason"],
                                }
                            ],
                        }

        async for chunk in process_stream():
            yield chunk

    async def _process_text_completion(self, request: TextCompletionRequest) -> CompletionResponse:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ text completion –∑–∞–ø—Ä–æ—Å–∞."""
        logger.debug("–û—Ç–ø—Ä–∞–≤–∫–∞ text completion –≤ LLM", prompt_len=len(request.prompt))

        response = self._llama.create_completion(
            prompt=request.prompt,
            max_tokens=request.max_tokens or 1024,
            temperature=request.temperature or 0.7,
            top_p=request.top_p or 0.9,
            top_k=request.top_k or 40,
            repeat_penalty=request.repeat_penalty or 1.1,
            seed=request.seed or -1,
            stream=False,
        )

        return self._create_completion_response(response, request.model, "text_completion")

    async def _process_text_completion_stream(self, request: TextCompletionRequest) -> AsyncIterator[dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ streaming text completion –∑–∞–ø—Ä–æ—Å–∞."""
        logger.debug("–û—Ç–ø—Ä–∞–≤–∫–∞ streaming text completion –≤ LLM", prompt_len=len(request.prompt))

        def create_stream():
            return self._llama.create_completion(
                prompt=request.prompt,
                max_tokens=request.max_tokens or 1024,
                temperature=request.temperature or 0.7,
                top_p=request.top_p or 0.9,
                top_k=request.top_k or 40,
                repeat_penalty=request.repeat_penalty or 1.1,
                seed=request.seed or -1,
                stream=True,
            )

        async def process_stream():
            stream = create_stream()
            
            for chunk in stream:
                if chunk and "choices" in chunk and chunk["choices"]:
                    choice = chunk["choices"][0]
                    if "text" in choice:
                        text = choice["text"]
                        if text:
                            yield {
                                "id": f"cmpl-{uuid.uuid4()}",
                                "object": "text_completion",
                                "created": int(time.time()),
                                "model": request.model,
                                "choices": [
                                    {
                                        "index": 0,
                                        "text": text,
                                        "finish_reason": None,
                                    }
                                ],
                            }
                    
                    if choice.get("finish_reason"):
                        yield {
                            "id": f"cmpl-{uuid.uuid4()}",
                            "object": "text_completion",
                            "created": int(time.time()),
                            "model": request.model,
                            "choices": [
                                {
                                    "index": 0,
                                    "text": "",
                                    "finish_reason": choice["finish_reason"],
                                }
                            ],
                        }

        async for chunk in process_stream():
            yield chunk

    async def is_ready(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–∞."""
        return self._llama is not None and self._rag_service is not None

    def _format_chat_messages(self, messages: list[ChatMessage]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è llama.cpp."""
        formatted_parts = []
        
        for message in messages:
            role = message.role
            content = message.content
            
            if role == "system":
                formatted_parts.append(f"System: {content}")
            elif role == "user":
                formatted_parts.append(f"User: {content}")
            elif role == "assistant":
                formatted_parts.append(f"Assistant: {content}")
        
        formatted_parts.append("Assistant:")
        return "\n\n".join(formatted_parts)

    def _create_completion_response(
        self, response: dict, model: str, object_type: str
    ) -> CompletionResponse:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ completion."""
        if not response or "choices" not in response:
            return self._create_error_response("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏")

        choice_data = response["choices"][0]
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
        if object_type == "chat.completion":
            content = choice_data.get("message", {}).get("content", "")
        else:
            content = choice_data.get("text", "")

        choice = Choice(
            index=0,
            message=ChatMessage(role="assistant", content=content),
            finish_reason=choice_data.get("finish_reason", "stop"),
        )

        usage = Usage(
            prompt_tokens=response.get("usage", {}).get("prompt_tokens", 0),
            completion_tokens=response.get("usage", {}).get("completion_tokens", 0),
            total_tokens=response.get("usage", {}).get("total_tokens", 0),
        )

        return CompletionResponse(
            id=f"chatcmpl-{uuid.uuid4()}",
            object=object_type,
            created=int(time.time()),
            model=model,
            choices=[choice],
            usage=usage,
        )

    def _create_error_response(self, error_message: str) -> CompletionResponse:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ —Å –æ—à–∏–±–∫–æ–π."""
        choice = Choice(
            index=0,
            message=ChatMessage(role="assistant", content=f"–û—à–∏–±–∫–∞: {error_message}"),
            finish_reason="stop",
        )

        return CompletionResponse(
            id=f"chatcmpl-{uuid.uuid4()}",
            object="chat.completion",
            created=int(time.time()),
            model="llama-cpp",
            choices=[choice],
            usage=Usage(prompt_tokens=0, completion_tokens=0, total_tokens=0),
        )

    def _create_stream_error(self, error_message: str) -> dict:
        """–°–æ–∑–¥–∞–Ω–∏–µ streaming –æ—à–∏–±–∫–∏."""
        return {
            "id": f"chatcmpl-{uuid.uuid4()}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": "llama-cpp",
            "choices": [
                {
                    "index": 0,
                    "delta": {"content": f"–û—à–∏–±–∫–∞: {error_message}"},
                    "finish_reason": "stop",
                }
            ],
        } 