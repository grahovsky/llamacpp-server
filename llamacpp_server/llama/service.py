"""–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLama.cpp."""

import time
import uuid
from typing import AsyncIterator

import structlog
from llama_cpp import Llama

from ..config.settings import get_settings
from ..domain.models import (
    ChatCompletionRequest,
    ChatMessage,
    Choice,
    CompletionResponse,
    TextCompletionRequest,
    Usage,
)
from .history_manager import ChatHistoryManager
from ..retrieval.protocols import RAGServiceProtocol


logger = structlog.get_logger(__name__)


class LlamaService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLama –º–æ–¥–µ–ª—å—é."""
    
    def __init__(
        self, 
        llama: Llama,
        rag_service: RAGServiceProtocol = None
    ) -> None:
        self._llama = llama
        self._rag_service = rag_service
        settings = get_settings()
        self._enable_rag = settings.enable_rag and rag_service is not None
        self._rag_search_k = settings.rag_search_k
        self._history_manager = ChatHistoryManager(
            llama_model=llama,
            max_tokens=settings.max_history_tokens,
            reserve_tokens=settings.context_reserve_tokens
        )
    
    @staticmethod
    def _is_technical_request(query: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã –æ—Ç open-webui."""
        if not query:
            return False
        
        # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        technical_patterns = [
            "### task:",
            "generate a concise",
            "generate 1-3 broad tags", 
            "summarizing the chat history",
            "categorizing the main themes",
            "json format:",
            '"title":', 
            '"tags":',
            "chat history:",
            "<chat_history>",
            "### guidelines:",
            "### output:",
            "3-5 word title",
            "broad tags categorizing",
            "main themes of the chat",
            "chat's primary language",
            "your entire response must consist solely",
            "raw json object",
        ]
        
        query_lower = query.lower()
        
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        has_technical_patterns = any(pattern in query_lower for pattern in technical_patterns)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å ### - —ç—Ç–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π
        starts_with_task = query_lower.strip().startswith("###")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        has_json_structure = '"title":' in query_lower or '"tags":' in query_lower
        
        return has_technical_patterns or starts_with_task or has_json_structure
        
    async def chat_completion(self, request: ChatCompletionRequest) -> CompletionResponse:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è chat completion —Å RAG –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π."""
        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ chat completion –∑–∞–ø—Ä–æ—Å–∞", 
                   model=request.model, 
                   rag_enabled=self._enable_rag)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        user_query = ""
        for msg in reversed(request.messages):
            if msg.role == "user":
                user_query = msg.content
                break

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
        is_technical = self._is_technical_request(user_query)
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
        if user_query:
            logger.info("üîç –¢–ò–ü –ó–ê–ü–†–û–°–ê",
                       is_technical=is_technical,
                       query_preview=user_query[:100])

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        has_system_message = any(msg.role == "system" for msg in request.messages)
        if not has_system_message and not is_technical:
            from ..prompts.templates import SYSTEM_PROMPTS
            system_message = ChatMessage(
                role="system", 
                content=SYSTEM_PROMPTS["rag_expert"]
            )
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –Ω–∞—á–∞–ª–æ
            request.messages.insert(0, system_message)
            logger.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è RAG", 
                       prompt_type="rag_expert")
        
        # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω RAG –∏ –µ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å (–Ω–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π)
        if self._enable_rag and user_query and not is_technical:
            logger.info("üß† RAG –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞", 
                       query_preview=user_query[:100],
                       rag_service_available=self._rag_service is not None)
            
            try:
                # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
                context = await self._rag_service.search_relevant_context(
                    user_query, k=self._rag_search_k
                )
                
                if context:
                    logger.info("üéØ RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞–π–¥–µ–Ω, —É–ª—É—á—à–∞–µ–º –ø—Ä–æ–º–ø—Ç", context_docs=len(context))
                    
                    # –£–ª—É—á—à–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                    enhanced_query = await self._rag_service.enhance_prompt_with_context(
                        user_query, context
                    )
                    
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
                    enhanced_messages = []
                    for msg in request.messages:
                        if msg.role == "user" and msg.content == user_query:
                            # –ó–∞–º–µ–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å
                            enhanced_messages.append(
                                ChatMessage(role="user", content=enhanced_query)
                            )
                        else:
                            enhanced_messages.append(msg)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
                    request = ChatCompletionRequest(
                        messages=enhanced_messages,
                        model=request.model,
                        max_tokens=request.max_tokens,
                        temperature=request.temperature,
                        top_p=request.top_p,
                        top_k=request.top_k,
                        repeat_penalty=request.repeat_penalty,
                        seed=request.seed,
                        stream=request.stream
                    )
                    
                    logger.info("‚úÖ RAG –ø—Ä–æ–º–ø—Ç –æ–±–Ω–æ–≤–ª–µ–Ω", 
                               original_length=len(user_query),
                               enhanced_length=len(enhanced_query),
                               context_docs=len(context))
                else:
                    logger.warning("‚ö†Ô∏è RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    
            except Exception as e:
                logger.error("‚ùå –û—à–∏–±–∫–∞ RAG –æ–±—Ä–∞–±–æ—Ç–∫–∏", error=str(e), exc_info=True)
        elif not self._enable_rag:
            logger.debug("RAG –æ—Ç–∫–ª—é—á–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö")
        else:
            logger.debug("–ù–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è RAG")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –ª–∏–º–∏—Ç–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        messages_dict = [msg.dict() for msg in request.messages]
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        logger.info("üîç –ò–°–•–û–î–ù–´–ï –°–û–û–ë–©–ï–ù–ò–Ø –î–õ–Ø LLM",
                   messages_count=len(messages_dict),
                   messages_summary=[{
                       "role": msg.get("role", "unknown"),
                       "content_length": len(str(msg.get("content", "")))
                   } for msg in messages_dict])
        
        # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –µ—Å–ª–∏ —ç—Ç–æ RAG (–±–µ–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö)
        if messages_dict and self._enable_rag and not is_technical:
            last_msg = messages_dict[-1]
            logger.debug("üîç –ü–û–°–õ–ï–î–ù–ï–ï –°–û–û–ë–©–ï–ù–ò–ï –° RAG (–ü–û–õ–ù–û–ï)",
                       role=last_msg.get("role", "unknown"),
                       full_content=last_msg.get("content", ""))
        
        trimmed_messages = self._history_manager.prepare_messages_for_completion(messages_dict)
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–µ–∑–∫–∏
        logger.info("üîç –°–û–û–ë–©–ï–ù–ò–Ø –ü–û–°–õ–ï –û–ë–†–ï–ó–ö–ò",
                   original_count=len(messages_dict),
                   trimmed_count=len(trimmed_messages),
                   trimmed_summary=[{
                       "role": msg.get("role", "unknown"),
                       "content_length": len(str(msg.get("content", "")))
                   } for msg in trimmed_messages])
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –ø—Ä–æ–º–ø—Ç
        formatted_prompt = self._format_chat_messages([
            ChatMessage(**msg) for msg in trimmed_messages
        ])
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç (—Ç–æ–ª—å–∫–æ –¥–ª–∏–Ω—É)
        logger.info("üîç –§–ò–ù–ê–õ–¨–ù–´–ô –ü–†–û–ú–ü–¢ –î–õ–Ø –ú–û–î–ï–õ–ò",
                   prompt_length=len(formatted_prompt))
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        if hasattr(self._llama, 'create_chat_completion'):
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º chat completion –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            response = self._llama.create_chat_completion(
                messages=trimmed_messages,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k,
                repeat_penalty=request.repeat_penalty,
                seed=request.seed,
                stream=False,
            )
        else:
            # Fallback –Ω–∞ –æ–±—ã—á–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
            response = self._llama.create_completion(
                prompt=formatted_prompt,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k,
                repeat_penalty=request.repeat_penalty,
                seed=request.seed,
                stream=False,
            )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –≤ OpenAI —Ñ–æ—Ä–º–∞—Ç–µ
        return self._create_completion_response(response, request.model, "chat.completion")
    
    async def chat_completion_stream(
        self, request: ChatCompletionRequest
    ) -> AsyncIterator[dict]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥ chat completion —Å RAG –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π."""
        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ streaming chat completion –∑–∞–ø—Ä–æ—Å–∞", 
                   model=request.model,
                   rag_enabled=self._enable_rag)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        user_query = ""
        for msg in reversed(request.messages):
            if msg.role == "user":
                user_query = msg.content
                break

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è streaming
        is_technical = self._is_technical_request(user_query)
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
        if user_query:
            logger.info("üîç STREAMING –¢–ò–ü –ó–ê–ü–†–û–°–ê",
                       is_technical=is_technical,
                       query_preview=user_query[:100])

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        has_system_message = any(msg.role == "system" for msg in request.messages)
        if not has_system_message and not is_technical:
            from ..prompts.templates import SYSTEM_PROMPTS
            system_message = ChatMessage(
                role="system", 
                content=SYSTEM_PROMPTS["rag_expert"]
            )
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –Ω–∞—á–∞–ª–æ
            request.messages.insert(0, system_message)
            logger.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è RAG streaming", 
                       prompt_type="rag_expert")
        
        # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω RAG –∏ –µ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å (–Ω–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π)
        if self._enable_rag and user_query and not is_technical:
            logger.info("üß† RAG –æ–±—Ä–∞–±–æ—Ç–∫–∞ streaming –∑–∞–ø—Ä–æ—Å–∞")
            
            try:
                # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
                context = await self._rag_service.search_relevant_context(
                    user_query, k=self._rag_search_k
                )
                
                if context:
                    # –£–ª—É—á—à–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                    enhanced_query = await self._rag_service.enhance_prompt_with_context(
                        user_query, context
                    )
                    
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
                    enhanced_messages = []
                    for msg in request.messages:
                        if msg.role == "user" and msg.content == user_query:
                            # –ó–∞–º–µ–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å
                            enhanced_messages.append(
                                ChatMessage(role="user", content=enhanced_query)
                            )
                        else:
                            enhanced_messages.append(msg)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
                    request = ChatCompletionRequest(
                        messages=enhanced_messages,
                        model=request.model,
                        max_tokens=request.max_tokens,
                        temperature=request.temperature,
                        top_p=request.top_p,
                        top_k=request.top_k,
                        repeat_penalty=request.repeat_penalty,
                        seed=request.seed,
                        stream=request.stream
                    )
                    
                    logger.info("‚úÖ RAG –ø—Ä–æ–º–ø—Ç –æ–±–Ω–æ–≤–ª–µ–Ω –¥–ª—è streaming", context_docs=len(context))
                    
            except Exception as e:
                logger.error("‚ùå –û—à–∏–±–∫–∞ RAG –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ streaming", error=str(e))
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –ª–∏–º–∏—Ç–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        messages_dict = [msg.dict() for msg in request.messages]
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è streaming —Ç–æ–∂–µ (–Ω–æ –∫–æ—Ä–æ—á–µ)
        logger.info("üîç STREAMING: –∏—Å—Ö–æ–¥–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è",
                   messages_count=len(messages_dict))
        
        if messages_dict and self._enable_rag:
            last_msg = messages_dict[-1]
            logger.info("üîç STREAMING: RAG —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª–∏–Ω–∞",
                       content_length=len(str(last_msg.get("content", ""))))
        
        trimmed_messages = self._history_manager.prepare_messages_for_completion(messages_dict)
        
        logger.info("üîç STREAMING: –ø–æ—Å–ª–µ –æ–±—Ä–µ–∑–∫–∏",
                   original_count=len(messages_dict),
                   trimmed_count=len(trimmed_messages))
        
        formatted_prompt = self._format_chat_messages([
            ChatMessage(**msg) for msg in trimmed_messages
        ])
        
        logger.info("üîç STREAMING: —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª–∏–Ω–∞",
                   prompt_length=len(formatted_prompt))
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å–∞–º –ø—Ä–æ–º–ø—Ç –¥–ª—è debug
        logger.info("üîç STREAMING: –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏",
                   prompt_preview=formatted_prompt[:500] + "..." if len(formatted_prompt) > 500 else formatted_prompt)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ thread pool —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
        import asyncio
        import concurrent.futures
        
        def create_stream():
            return self._llama.create_completion(
                prompt=formatted_prompt,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k,
                repeat_penalty=request.repeat_penalty,
                seed=request.seed,
                stream=True,
            )
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º stream –≤ executor
        loop = asyncio.get_event_loop()
        
        async def process_stream():
            # –ü–æ–ª—É—á–∞–µ–º stream
            stream = create_stream()
            
            # –ï—Å–ª–∏ —ç—Ç–æ list (mock), –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏
            if isinstance(stream, list):
                for chunk in stream:
                    await asyncio.sleep(0.2)  # –≠–º—É–ª–∏—Ä—É–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                    
                    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
                    if "choices" in chunk and chunk["choices"]:
                        choice = chunk["choices"][0]
                        if "delta" in choice and choice["delta"]:
                            yield chunk
                        elif "text" in choice:
                            yield {
                                "choices": [{
                                    "delta": {"content": choice["text"]},
                                    "finish_reason": choice.get("finish_reason")
                                }]
                            }
                        else:
                            yield chunk
                    else:
                        yield chunk
            else:
                # –†–µ–∞–ª—å–Ω—ã–π stream –æ—Ç llama.cpp
                def get_next_chunk(stream_iter):
                    try:
                        return next(stream_iter)
                    except StopIteration:
                        return None
                
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    stream_iter = iter(stream)
                    
                    while True:
                        # –ü–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π —á–∞–Ω–∫ –≤ thread pool
                        chunk = await loop.run_in_executor(executor, get_next_chunk, stream_iter)
                        
                        if chunk is None:
                            break
                        
                        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
                        if "choices" in chunk and chunk["choices"]:
                            choice = chunk["choices"][0]
                            if "delta" in choice and choice["delta"]:
                                yield chunk
                            elif "text" in choice:
                                yield {
                                    "choices": [{
                                        "delta": {"content": choice["text"]},
                                        "finish_reason": choice.get("finish_reason")
                                    }]
                                }
                            else:
                                yield chunk
                        else:
                            yield chunk
        
        async for chunk in process_stream():
            yield chunk
    
    async def text_completion(self, request: TextCompletionRequest) -> CompletionResponse:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è text completion."""
        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ text completion –∑–∞–ø—Ä–æ—Å–∞", model=request.model)
        
        response = self._llama.create_completion(
            prompt=request.prompt,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
            top_k=request.top_k,
            repeat_penalty=request.repeat_penalty,
            seed=request.seed,
            stream=False,
        )
        
        return self._create_completion_response(response, request.model, "text_completion")
    
    async def text_completion_stream(
        self, request: TextCompletionRequest
    ) -> AsyncIterator[dict]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥ text completion."""
        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ streaming text completion –∑–∞–ø—Ä–æ—Å–∞", model=request.model)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ thread pool —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
        import asyncio
        import concurrent.futures
        
        def create_stream():
            return self._llama.create_completion(
                prompt=request.prompt,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k,
                repeat_penalty=request.repeat_penalty,
                seed=request.seed,
                stream=True,
            )
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º stream –≤ executor
        loop = asyncio.get_event_loop()
        
        async def process_stream():
            # –ü–æ–ª—É—á–∞–µ–º stream
            stream = create_stream()
            
            # –ï—Å–ª–∏ —ç—Ç–æ list (mock), –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏
            if isinstance(stream, list):
                for chunk in stream:
                    await asyncio.sleep(0.2)  # –≠–º—É–ª–∏—Ä—É–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                    
                    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ–º —Ñ–æ—Ä–º–∞—Ç —á–∞–Ω–∫–æ–≤
                    if "choices" in chunk and chunk["choices"]:
                        choice = chunk["choices"][0]
                        if "delta" in choice and choice["delta"]:
                            yield chunk
                        elif "text" in choice:
                            yield {
                                "choices": [{
                                    "delta": {"content": choice["text"]},
                                    "finish_reason": choice.get("finish_reason"),
                                    "text": choice["text"]
                                }]
                            }
                        else:
                            yield chunk
                    else:
                        yield chunk
            else:
                # –†–µ–∞–ª—å–Ω—ã–π stream –æ—Ç llama.cpp
                def get_next_chunk(stream_iter):
                    try:
                        return next(stream_iter)
                    except StopIteration:
                        return None
                
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    stream_iter = iter(stream)
                    
                    while True:
                        # –ü–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π —á–∞–Ω–∫ –≤ thread pool
                        chunk = await loop.run_in_executor(executor, get_next_chunk, stream_iter)
                        
                        if chunk is None:
                            break
                        
                        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ–º —Ñ–æ—Ä–º–∞—Ç —á–∞–Ω–∫–æ–≤
                        if "choices" in chunk and chunk["choices"]:
                            choice = chunk["choices"][0]
                            if "delta" in choice and choice["delta"]:
                                yield chunk
                            elif "text" in choice:
                                yield {
                                    "choices": [{
                                        "delta": {"content": choice["text"]},
                                        "finish_reason": choice.get("finish_reason"),
                                        "text": choice["text"]
                                    }]
                                }
                            else:
                                yield chunk
                        else:
                            yield chunk
        
        async for chunk in process_stream():
            yield chunk
    
    async def is_ready(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."""
        return True  # LLama –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    
    def _format_chat_messages(self, messages: list[ChatMessage]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –ø—Ä–æ–º–ø—Ç –¥–ª—è Mistral-7B."""
        # –£–±–∏—Ä–∞–µ–º <s> - llama.cpp –¥–æ–±–∞–≤–ª—è–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        formatted = ""
        system_content = ""
        user_content = ""
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
        for msg in messages:
            if msg.role == "system":
                system_content = msg.content
            elif msg.role == "user":
                user_content = msg.content
            elif msg.role == "assistant":
                # –î–ª—è –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞ (–ø–æ–∫–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º)
                pass
        
        # –§–æ—Ä–º–∞—Ç Mistral –±–µ–∑ <s> (llama.cpp –¥–æ–±–∞–≤–∏—Ç —Å–∞–º)
        if system_content and user_content:
            formatted = f"[INST] <<SYS>>\n{system_content}\n<</SYS>>\n\n{user_content} [/INST]"
        elif user_content:
            formatted = f"[INST] {user_content} [/INST]"
        
        return formatted
    
    def _create_completion_response(
        self, response: dict, model: str, object_type: str
    ) -> CompletionResponse:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –≤ OpenAI —Ñ–æ—Ä–º–∞—Ç–µ."""
        choice = response["choices"][0]
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –æ—Ç–≤–µ—Ç–æ–≤
        if object_type == "chat.completion" and "message" in choice:
            # Chat completion –æ—Ç–≤–µ—Ç
            message_content = choice["message"]["content"]
            choice_obj = Choice(
                index=0,
                message=ChatMessage(role="assistant", content=message_content),
                finish_reason=choice.get("finish_reason", "stop"),
            )
        else:
            # Text completion –æ—Ç–≤–µ—Ç
            text_content = choice.get("text", "")
            choice_obj = Choice(
                index=0,
                text=text_content,
                message=(
                    ChatMessage(role="assistant", content=text_content)
                    if object_type == "chat.completion"
                    else None
                ),
                finish_reason=choice.get("finish_reason", "stop"),
            )
        
        return CompletionResponse(
            id=f"cmpl-{uuid.uuid4().hex}",
            object=object_type,
            created=int(time.time()),
            model=model,
            choices=[choice_obj],
            usage=Usage(
                prompt_tokens=response["usage"]["prompt_tokens"],
                completion_tokens=response["usage"]["completion_tokens"],
                total_tokens=response["usage"]["total_tokens"],
            ),
        ) 