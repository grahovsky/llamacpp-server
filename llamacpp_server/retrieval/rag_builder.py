"""–ú–æ–¥—É–ª—å –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è RAG –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã."""

import json
import re
import uuid
from pathlib import Path

import faiss
import numpy as np
import structlog
from sentence_transformers import SentenceTransformer

from .protocols import Document
from .semantic_chunker import SemanticChunker

logger = structlog.get_logger(__name__)


def split_text_with_overlap(text: str, chunk_size: int = 1500, overlap_size: int = 200) -> list[str]:
    """–†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º."""
    if len(text) <= chunk_size:
        return [text]

    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size

        # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫, –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ö–æ—Ä–æ—à–µ–µ –º–µ—Å—Ç–æ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        if end < len(text):
            # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å (—Ç–æ—á–∫–∞, –Ω–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞, –ø—Ä–æ–±–µ–ª)
            good_split = -1
            for i in range(end - 100, end):
                if i >= 0 and text[i] in '.!?\n':
                    good_split = i + 1
                    break

            if good_split != -1:
                end = good_split

        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)

        # –°–ª–µ–¥—É—é—â–∏–π —á–∞–Ω–∫ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
        start = max(start + chunk_size - overlap_size, end)

        if start >= len(text):
            break

    return chunks


def preprocess_text(text: str) -> str:
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞."""
    if not text:
        return ""

    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    text = text.replace("false", "").replace("true", "")

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()

    return text


class RAGBuilder:
    """–ü–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å RAG –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã."""

    def __init__(
        self,
        model_name: str = "BAAI/bge-m3",
        embedding_dim: int = 1024,
        chunk_size: int = 1500,
        overlap_size: int = 200,
        batch_size: int = 32,
        use_semantic_chunking: bool = True,
        similarity_threshold: float = 0.7
    ):
        self.model_name = model_name
        self.embedding_dim = embedding_dim
        self.chunk_size = chunk_size
        self.overlap_size = overlap_size
        self.batch_size = batch_size
        self.use_semantic_chunking = use_semantic_chunking
        self.similarity_threshold = similarity_threshold
        self.embedding_model = None
        self.semantic_chunker = None

    async def build_from_confluence_data(
        self,
        input_file: Path,
        output_dir: Path
    ) -> None:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å RAG –±–∞–∑—É –∏–∑ –¥–∞–Ω–Ω—ã—Ö Confluence."""
        logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ RAG –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã")

        # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        output_dir.mkdir(parents=True, exist_ok=True)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        logger.info("üìñ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Confluence", file=str(input_file))
        with open(input_file, encoding='utf-8') as f:
            confluence_data = json.load(f)

        logger.info("üìä –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã", total_pages=len(confluence_data))

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–∞ CPU
        logger.info("üß† –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–∞ CPU", model=self.model_name)
        device = "cpu"  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU
        self.embedding_model = SentenceTransformer(self.model_name)
        self.embedding_model.to(device)
        logger.info("‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ CPU")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–±–∏–≤—â–∏–∫ —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ç–µ—Ö–Ω–∏–∫–∞–º–∏
        if self.use_semantic_chunking:
            self.semantic_chunker = SemanticChunker(
                embedding_model=self.embedding_model,
                max_chunk_size=self.chunk_size,
                min_chunk_size=100,
                similarity_threshold=self.similarity_threshold,
                overlap_sentences=2,
                window_size=3,              # –°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–≤—è–∑–Ω–æ—Å—Ç–∏
                adaptive_threshold=True,    # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                hierarchical_chunking=True  # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            )
            logger.info("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–±–∏–≤—â–∏–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –Ω–æ–≤—ã–º–∏ —Ç–µ—Ö–Ω–∏–∫–∞–º–∏")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        documents = await self._process_documents(confluence_data)

        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        embeddings = await self._create_embeddings(documents, device)

        # –°–æ–∑–¥–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º FAISS –∏–Ω–¥–µ–∫—Å
        await self._create_and_save_index(documents, embeddings, output_dir)

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
        await self._test_search(embeddings, documents, device)

    async def _process_documents(self, confluence_data: list[dict]) -> list[Document]:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ Confluence."""
        documents = []
        doc_count = 0

        for page in confluence_data:
            content = page.get("page_content", "")
            metadata = page.get("metadata", {})

            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç
            content = preprocess_text(content)

            if not content or len(content) < 50:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                logger.debug("–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç", title=metadata.get("title", ""))
                continue

            title = metadata.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
            source = metadata.get("source", "")
            page_id = metadata.get("id", "")

            # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
            base_metadata = {
                "title": title,
                "source": source,
                "page_id": page_id,
                "when": metadata.get("when", "")
            }

            # –í—ã–±–∏—Ä–∞–µ–º —Ç–∏–ø —Ä–∞–∑–±–∏–≤–∫–∏
            if self.use_semantic_chunking and self.semantic_chunker:
                logger.debug("–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ä–∞–∑–±–∏–≤–∫—É", title=title)
                chunk_dicts = await self.semantic_chunker.chunk_text(content, base_metadata)
                chunks = [chunk_dict["content"] for chunk_dict in chunk_dicts]
                chunks_metadata = [chunk_dict["metadata"] for chunk_dict in chunk_dicts]
            else:
                logger.debug("–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é —Ä–∞–∑–±–∏–≤–∫—É", title=title)
                chunks = split_text_with_overlap(content, self.chunk_size, self.overlap_size)
                chunks_metadata = []
                for i, chunk in enumerate(chunks):
                    chunk_metadata = base_metadata.copy()
                    chunk_metadata.update({
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                        "chunk_type": "overlap"
                    })
                    chunks_metadata.append(chunk_metadata)

            logger.debug("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç",
                        title=title,
                        original_length=len(content),
                        chunks_count=len(chunks),
                        chunking_type="semantic" if self.use_semantic_chunking else "overlap")

            for i, (chunk, chunk_metadata) in enumerate(zip(chunks, chunks_metadata, strict=False)):
                # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è —á–∞–Ω–∫–∞
                chunk_id = f"{page_id}_{i}" if page_id else str(uuid.uuid4())

                # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
                doc = Document(
                    id=chunk_id,
                    content=chunk,
                    metadata=chunk_metadata
                )

                documents.append(doc)
                doc_count += 1

        logger.info("üìÑ –î–æ–∫—É–º–µ–Ω—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã",
                   total_documents=doc_count,
                   from_pages=len([p for p in confluence_data if p.get("page_content")]),
                   chunking_method="semantic" if self.use_semantic_chunking else "overlap")

        return documents

    async def _create_embeddings(self, documents: list[Document], device: str) -> list[np.ndarray]:
        """–°–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        logger.info("üî¢ –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        texts = [doc.content for doc in documents]

        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–∞—Ç—á–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        embeddings = []

        for i in range(0, len(texts), self.batch_size):
            batch_texts = texts[i:i + self.batch_size]
            logger.debug("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á", batch=f"{i//self.batch_size + 1}/{(len(texts)-1)//self.batch_size + 1}")

            batch_embeddings = self.embedding_model.encode(
                batch_texts,
                convert_to_tensor=False,
                show_progress_bar=True,
                device=device
            )
            embeddings.extend(batch_embeddings)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        for doc, embedding in zip(documents, embeddings, strict=False):
            doc.embedding = embedding.tolist()

        logger.info("‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã", total_embeddings=len(embeddings))
        return embeddings

    async def _create_and_save_index(
        self,
        documents: list[Document],
        embeddings: list[np.ndarray],
        output_dir: Path
    ) -> None:
        """–°–æ–∑–¥–∞—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å FAISS –∏–Ω–¥–µ–∫—Å."""
        logger.info("üóÑÔ∏è –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å")

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ numpy array
        embeddings_array = np.array(embeddings, dtype=np.float32)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è cosine similarity
        faiss.normalize_L2(embeddings_array)

        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
        index = faiss.IndexFlatIP(self.embedding_dim)  # Inner product –¥–ª—è cosine similarity
        index.add(embeddings_array)

        logger.info("‚úÖ FAISS –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω", vectors_count=index.ntotal)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
        index_file = output_dir / "index.faiss"
        docs_file = output_dir / "documents.json"

        logger.info("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å –∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º FAISS –∏–Ω–¥–µ–∫—Å
        faiss.write_index(index, str(index_file))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        docs_data = {}
        for i, doc in enumerate(documents):
            docs_data[str(i)] = {
                "id": doc.id,
                "content": doc.content,
                "metadata": doc.metadata,
                "embedding": doc.embedding
            }

        with open(docs_file, 'w', encoding='utf-8') as f:
            json.dump(docs_data, f, ensure_ascii=False, indent=2)

        logger.info("üéâ RAG –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞!",
                   index_file=str(index_file),
                   docs_file=str(docs_file),
                   total_vectors=index.ntotal)

    async def _test_search(
        self,
        embeddings: list[np.ndarray],
        documents: list[Document],
        device: str
    ) -> None:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –≤ —Å–æ–∑–¥–∞–Ω–Ω–æ–π –±–∞–∑–µ."""
        logger.info("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫...")

        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        embeddings_array = np.array(embeddings, dtype=np.float32)
        faiss.normalize_L2(embeddings_array)
        test_index = faiss.IndexFlatIP(self.embedding_dim)
        test_index.add(embeddings_array)

        test_query = "Jenkins URL"
        test_embedding = self.embedding_model.encode([test_query], convert_to_tensor=False, device=device)
        test_embedding = np.array(test_embedding, dtype=np.float32)
        faiss.normalize_L2(test_embedding)

        scores, indices = test_index.search(test_embedding, 5)

        logger.info("üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞", query=test_query)
        for i, (score, idx) in enumerate(zip(scores[0], indices[0], strict=False)):
            if idx != -1:
                doc = documents[idx]
                logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç {i+1}",
                           score=float(score),
                           title=doc.metadata.get("title", ""),
                           content_preview=doc.content[:200] + "...")


async def build_rag_from_confluence(
    input_file: str = "data/confluence_data.json",
    output_dir: str = "data/faiss_index"
) -> None:
    """–£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è RAG –±–∞–∑—ã."""
    builder = RAGBuilder()
    await builder.build_from_confluence_data(
        Path(input_file),
        Path(output_dir)
    )
