"""FAISS –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ."""

import asyncio
import json
import pickle
from pathlib import Path

import faiss
import numpy as np
import structlog

from ..config.settings import get_settings
from .protocols import Document, SearchResult

logger = structlog.get_logger(__name__)


class FaissVectorStore:
    """FAISS –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å async –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º."""

    def __init__(self) -> None:
        self._index: faiss.Index = None
        self._documents: dict[int, Document] = {}
        self._dimension: int = 1024  # BGE-M3 dimension
        settings = get_settings()
        self._index_dir = Path(settings.faiss_index_path)

    async def _ensure_index_loaded(self) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏."""
        if self._index is None:
            logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞", index_dir=str(self._index_dir))

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∞–π–ª–æ–≤
            possible_files = [
                # –ù–∞—à –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
                (self._index_dir / "index.faiss", self._index_dir / "documents.json"),
            ]

            index_loaded = False
            for index_file, docs_file in possible_files:
                logger.debug("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤", index_file=str(index_file), docs_file=str(docs_file))

                if index_file.exists():
                    logger.info("–ù–∞–π–¥–µ–Ω FAISS –∏–Ω–¥–µ–∫—Å",
                               index_file=str(index_file),
                               docs_file=str(docs_file),
                               docs_exists=docs_file.exists())

                    try:
                        await self._load_index_files(index_file, docs_file)
                        index_loaded = True
                        break
                    except Exception as e:
                        logger.warning("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞",
                                     index_file=str(index_file),
                                     error=str(e))
                        continue

            if not index_loaded:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
                logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ FAISS –∏–Ω–¥–µ–∫—Å–∞", dimension=self._dimension)
                loop = asyncio.get_event_loop()
                self._index = await loop.run_in_executor(
                    None,
                    lambda: faiss.IndexFlatIP(self._dimension)  # Inner product –¥–ª—è cosine similarity
                )

    async def add_documents(self, documents: list[Document], batch_size: int = 100) -> None:
        """–î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π."""
        await self._ensure_index_loaded()

        logger.info("üîÑ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ FAISS —Å –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π", 
                   count=len(documents), 
                   batch_size=batch_size)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –±–∞—Ç—á–∞–º–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
        total_added = 0
        for i in range(0, len(documents), batch_size):
            batch_docs = documents[i : i + batch_size]
            batch_end = min(i + batch_size, len(documents))
            
            logger.debug("–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞", 
                        batch=f"{i + 1}-{batch_end}",
                        total=len(documents))

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –±–∞—Ç—á–∞
            embeddings = []
            for doc in batch_docs:
                if doc.embedding is None:
                    raise ValueError(f"Document {doc.id} missing embedding")
                embeddings.append(doc.embedding)

            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è cosine similarity
            embeddings_array = np.array(embeddings, dtype=np.float32)
            faiss.normalize_L2(embeddings_array)

            # –î–æ–±–∞–≤–ª—è–µ–º –±–∞—Ç—á –≤ –∏–Ω–¥–µ–∫—Å
            loop = asyncio.get_event_loop()
            current_size = self._index.ntotal

            await loop.run_in_executor(
                None,
                self._index.add,
                embeddings_array
            )

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –±–∞—Ç—á–∞
            for j, doc in enumerate(batch_docs):
                self._documents[current_size + j] = doc

            total_added += len(batch_docs)
            logger.debug("‚úÖ –ë–∞—Ç—á –æ–±—Ä–∞–±–æ—Ç–∞–Ω",
                        batch_docs=len(batch_docs),
                        total_added=total_added,
                        total_in_index=self._index.ntotal)

        logger.info("üéâ –í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–æ–±–∞–≤–ª–µ–Ω—ã", 
                   total_docs=self._index.ntotal,
                   batches_processed=len(range(0, len(documents), batch_size)))

    async def search(self, query_embedding: list[float], k: int = 5) -> list[SearchResult]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        await self._ensure_index_loaded()

        if self._index.ntotal == 0:
            logger.warning("–ü–æ–∏—Å–∫ –≤ –ø—É—Å—Ç–æ–º –∏–Ω–¥–µ–∫—Å–µ")
            return []

        logger.debug("–ü–æ–∏—Å–∫ –≤ FAISS", k=k, total_docs=self._index.ntotal)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è cosine similarity
        query_array = np.array([query_embedding], dtype=np.float32)
        faiss.normalize_L2(query_array)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –≤ thread pool
        loop = asyncio.get_event_loop()
        scores, indices = await loop.run_in_executor(
            None,
            self._index.search,
            query_array,
            k
        )

        # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        logger.debug("üîç FAISS Search Results",
                    raw_scores=scores[0].tolist() if len(scores) > 0 else [],
                    raw_indices=indices[0].tolist() if len(indices) > 0 else [],
                    available_docs=list(self._documents.keys())[:10])  # –ü–µ—Ä–≤—ã–µ 10 –∫–ª—é—á–µ–π –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = []
        for score, idx in zip(scores[0], indices[0], strict=False):
            if idx == -1:  # FAISS –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç -1 –¥–ª—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                logger.debug("–ü—Ä–æ–ø—É—Å–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –∏–Ω–¥–µ–∫—Å–æ–º -1")
                continue
            if idx in self._documents:
                doc = self._documents[idx]
                result = SearchResult(document=doc, score=float(score))
                results.append(result)

                logger.debug("–ù–∞–π–¥–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç",
                           doc_id=doc.id,
                           score=float(score),
                           content_preview=doc.content[:200] + "..." if len(doc.content) > 200 else doc.content)
            else:
                logger.warning("–ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö", idx=int(idx), available_keys=list(self._documents.keys())[:5])

        logger.debug("–ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω", found=len(results))
        return results

    async def _load_index_files(self, index_file: Path, docs_file: Path) -> None:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤."""
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞",
                   index_file=str(index_file),
                   docs_file=str(docs_file))

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å –≤ thread pool
        loop = asyncio.get_event_loop()
        self._index = await loop.run_in_executor(
            None,
            faiss.read_index,
            str(index_file)
        )

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞
        if docs_file.exists():
            if docs_file.suffix == '.json':
                # –ù–∞—à –Ω–æ–≤—ã–π JSON —Ñ–æ—Ä–º–∞—Ç
                with open(docs_file, encoding='utf-8') as f:
                    docs_data = json.load(f)

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö
                    logger.debug("–ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
                                sample_keys=list(docs_data.keys())[:5],
                                total_keys=len(docs_data))

                    if isinstance(docs_data, dict) and all(isinstance(v, dict) for v in docs_data.values()):
                        # –≠—Ç–æ –Ω–∞—à –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç {index: {id, content, metadata, embedding}}
                        self._documents = {}
                        for i, doc_data in docs_data.items():
                            self._documents[int(i)] = Document(
                                id=doc_data["id"],
                                content=doc_data["content"],
                                metadata=doc_data["metadata"],
                                embedding=doc_data.get("embedding")
                            )
                        logger.info("–ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –Ω–æ–≤–æ–≥–æ JSON —Ñ–æ—Ä–º–∞—Ç–∞", count=len(self._documents))
                    else:
                        # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç
                        self._documents = {
                            int(k): Document(**v) for k, v in docs_data.items()
                        }
                        logger.info("–ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ JSON —Ñ–æ—Ä–º–∞—Ç–∞", count=len(self._documents))
            elif docs_file.suffix == '.pkl':
                # Pickle —Ñ–æ—Ä–º–∞—Ç (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –¥–ª—è –º–Ω–æ–≥–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫)
                with open(docs_file, 'rb') as f:
                    docs_data = pickle.load(f)

                    logger.info("–ê–Ω–∞–ª–∏–∑ pickle –¥–∞–Ω–Ω—ã—Ö",
                               type=type(docs_data).__name__,
                               has_keys=hasattr(docs_data, 'keys'),
                               length=len(docs_data) if hasattr(docs_data, '__len__') else 'N/A')

                    # –ï—Å–ª–∏ —ç—Ç–æ LangChain –æ–±—ä–µ–∫—Ç, –ø–æ–ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –¥–æ–∫—É–º–µ–Ω—Ç—ã
                    if hasattr(docs_data, 'docstore') and hasattr(docs_data.docstore, '_dict'):
                        logger.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω LangChain docstore")
                        langchain_docs = docs_data.docstore._dict
                        self._documents = {}
                        for i, (doc_id, doc) in enumerate(langchain_docs.items()):
                            if hasattr(doc, 'page_content'):
                                self._documents[i] = Document(
                                    id=str(doc_id),
                                    content=doc.page_content,
                                    metadata=getattr(doc, 'metadata', {}),
                                    embedding=None
                                )
                            else:
                                self._documents[i] = Document(
                                    id=str(doc_id),
                                    content=str(doc),
                                    metadata={},
                                    embedding=None
                                )
                        logger.info("–ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ LangChain", count=len(self._documents))

                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã pickle –¥–∞–Ω–Ω—ã—Ö
                    elif isinstance(docs_data, dict):
                        # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                        self._documents = {}
                        for i, (doc_id, content) in enumerate(docs_data.items()):
                            if isinstance(content, dict):
                                # –ï—Å–ª–∏ content —É–∂–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
                                self._documents[i] = Document(
                                    id=str(doc_id),
                                    content=content.get('content', str(content)),
                                    metadata=content.get('metadata', {}),
                                    embedding=content.get('embedding')
                                )
                            else:
                                # –ï—Å–ª–∏ content —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç
                                self._documents[i] = Document(
                                    id=str(doc_id),
                                    content=str(content),
                                    metadata={},
                                    embedding=None
                                )
                    elif isinstance(docs_data, list):
                        # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                        self._documents = {}
                        for i, doc in enumerate(docs_data):
                            if isinstance(doc, dict):
                                self._documents[i] = Document(
                                    id=doc.get('id', str(i)),
                                    content=doc.get('content', str(doc)),
                                    metadata=doc.get('metadata', {}),
                                    embedding=doc.get('embedding')
                                )
                            else:
                                self._documents[i] = Document(
                                    id=str(i),
                                    content=str(doc),
                                    metadata={},
                                    embedding=None
                                )
                    elif isinstance(docs_data, tuple):
                        # Tuple —Ñ–æ—Ä–º–∞—Ç - —á–∞—Å—Ç–æ —ç—Ç–æ (index, docstore) –∏–ª–∏ (docs, embeddings)
                        logger.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω tuple —Ñ–æ—Ä–º–∞—Ç", items=len(docs_data))

                        # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
                        for i, item in enumerate(docs_data):
                            logger.info(f"Tuple —ç–ª–µ–º–µ–Ω—Ç {i}",
                                       type=type(item).__name__,
                                       has_len=hasattr(item, '__len__'),
                                       length=len(item) if hasattr(item, '__len__') else 'N/A')

                        # –ï—Å–ª–∏ —ç—Ç–æ LangChain FAISS —Ñ–æ—Ä–º–∞—Ç (vectorstore, docstore)
                        if len(docs_data) >= 2:
                            # –û–±—ã—á–Ω–æ –≤—Ç–æ—Ä–æ–π —ç–ª–µ–º–µ–Ω—Ç - —ç—Ç–æ docstore
                            possible_docstore = docs_data[1] if len(docs_data) > 1 else docs_data[0]

                            if hasattr(possible_docstore, '_dict'):
                                logger.info("–ù–∞–π–¥–µ–Ω docstore –≤ tuple")
                                langchain_docs = possible_docstore._dict
                                self._documents = {}
                                for i, (doc_id, doc) in enumerate(langchain_docs.items()):
                                    if hasattr(doc, 'page_content'):
                                        self._documents[i] = Document(
                                            id=str(doc_id),
                                            content=doc.page_content,
                                            metadata=getattr(doc, 'metadata', {}),
                                            embedding=None
                                        )
                                    else:
                                        self._documents[i] = Document(
                                            id=str(doc_id),
                                            content=str(doc),
                                            metadata={},
                                            embedding=None
                                        )
                                logger.info("–ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ tuple docstore", count=len(self._documents))
                            elif isinstance(possible_docstore, (list, dict)):
                                # –ü—Ä—è–º–æ–π —Å–ø–∏—Å–æ–∫/—Å–ª–æ–≤–∞—Ä—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                                self._documents = {}
                                if isinstance(possible_docstore, dict):
                                    docs_iter = possible_docstore.items()
                                else:
                                    docs_iter = enumerate(possible_docstore)

                                for i, (doc_id, doc) in enumerate(docs_iter):
                                    if hasattr(doc, 'page_content'):
                                        content = doc.page_content
                                        metadata = getattr(doc, 'metadata', {})
                                    elif isinstance(doc, dict):
                                        content = doc.get('content', doc.get('text', str(doc)))
                                        metadata = doc.get('metadata', {})
                                    else:
                                        content = str(doc)
                                        metadata = {}

                                    self._documents[i] = Document(
                                        id=str(doc_id),
                                        content=content,
                                        metadata=metadata,
                                        embedding=None
                                    )
                                logger.info("–ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ tuple list/dict", count=len(self._documents))
                            else:
                                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ tuple")
                                self._documents = {}
                        else:
                            logger.warning("Tuple —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π")
                            self._documents = {}
                    else:
                        logger.warning("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç pickle –¥–∞–Ω–Ω—ã—Ö", type=type(docs_data))
                        self._documents = {}
        else:
            logger.warning("–§–∞–π–ª –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—é –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å")
            self._documents = {}

        logger.info("FAISS –∏–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω",
                   docs_count=len(self._documents),
                   index_size=self._index.ntotal)

    async def load_index(self, index_path: str) -> None:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å."""
        index_dir = Path(index_path)
        index_file = index_dir / "index.faiss"  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –∏–º—è —Ñ–∞–π–ª–∞
        docs_file = index_dir / "documents.json"

        if not index_file.exists() or not docs_file.exists():
            raise FileNotFoundError(f"Index files not found in {index_path}")

        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞", path=index_path)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å –≤ thread pool
        loop = asyncio.get_event_loop()
        self._index = await loop.run_in_executor(
            None,
            faiss.read_index,
            str(index_file)
        )

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –Ω–∞—à–µ–º —Ñ–æ—Ä–º–∞—Ç–µ
        with open(docs_file, encoding='utf-8') as f:
            docs_data = json.load(f)
            self._documents = {}
            for i, doc_data in docs_data.items():
                self._documents[int(i)] = Document(
                    id=doc_data["id"],
                    content=doc_data["content"],
                    metadata=doc_data["metadata"],
                    embedding=doc_data.get("embedding")
                )

        logger.info("FAISS –∏–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω",
                   docs_count=len(self._documents),
                   index_size=self._index.ntotal)

    async def save_index(self, index_path: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω–¥–µ–∫—Å."""
        if self._index is None:
            logger.warning("–ü–æ–ø—ã—Ç–∫–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—É—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å")
            return

        index_dir = Path(index_path)
        index_dir.mkdir(parents=True, exist_ok=True)

        index_file = index_dir / "faiss.index"
        docs_file = index_dir / "documents.json"

        logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞", path=index_path)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å –≤ thread pool
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            None,
            faiss.write_index,
            self._index,
            str(index_file)
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        docs_data = {
            str(k): {
                "id": v.id,
                "content": v.content,
                "metadata": v.metadata,
                "embedding": v.embedding
            }
            for k, v in self._documents.items()
        }

        with open(docs_file, 'w', encoding='utf-8') as f:
            json.dump(docs_data, f, ensure_ascii=False, indent=2)

        logger.info("FAISS –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
