"""–°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–º–µ–Ω–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º chonkie –∏ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤."""

import re
import uuid
from enum import Enum
from typing import Any

import structlog
from chonkie import SentenceChunker, TokenChunker, RecursiveChunker

logger = structlog.get_logger(__name__)


class ChunkingStrategy(Enum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ —á–∞–Ω–∫–∏–Ω–≥–∞."""
    SEMANTIC = "semantic"
    TITLE_BASED = "title"
    HYBRID = "hybrid"


class ModernSemanticChunker:
    """–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫–µ—Ä —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º chonkie –∏ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤."""

    def __init__(
        self,
        strategy: ChunkingStrategy = ChunkingStrategy.HYBRID,
        chunk_size: int = 512,
        overlap: int = 50,
        min_chunk_size: int = 100,
        similarity_threshold: float = 0.7
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —á–∞–Ω–∫–µ—Ä–∞.

        Args:
            strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–∏–Ω–≥–∞
            chunk_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
            overlap: –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
            min_chunk_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
            similarity_threshold: –ü–æ—Ä–æ–≥ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
        """
        self.strategy = strategy
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.min_chunk_size = min_chunk_size
        self.similarity_threshold = similarity_threshold

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞–Ω–∫–µ—Ä—ã –∏–∑ chonkie
        self.token_chunker = TokenChunker(
            chunk_size=chunk_size,
            chunk_overlap=overlap
        )

        self.recursive_chunker = RecursiveChunker(
            chunk_size=chunk_size
        )

        self.sentence_chunker = SentenceChunker(
            chunk_size=chunk_size,
            chunk_overlap=overlap
        )

        self.initialized = False

        logger.info("–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —á–∞–Ω–∫–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω",
                   strategy=strategy.value,
                   chunk_size=chunk_size,
                   overlap=overlap)

    async def initialize(self) -> None:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è."""
        if self.initialized:
            return

        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —á–∞–Ω–∫–µ—Ä–∞")

        # –ó–¥–µ—Å—å –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        # –ù–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞

        self.initialized = True
        logger.info("‚úÖ –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —á–∞–Ω–∫–µ—Ä –≥–æ—Ç–æ–≤")

    async def chunk_document(
        self,
        content: str,
        metadata: dict[str, Any] | None = None
    ) -> list[dict[str, Any]]:
        """
        –ß–∞–Ω–∫–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π.

        Args:
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        await self.initialize()

        if not content or len(content) < self.min_chunk_size:
            logger.warning("–î–æ–∫—É–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞", length=len(content))
            return []

        logger.info("–ù–∞—á–∏–Ω–∞–µ–º —á–∞–Ω–∫–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                   strategy=self.strategy.value,
                   content_length=len(content))

        if self.strategy == ChunkingStrategy.SEMANTIC:
            return await self._chunk_semantic(content, metadata)
        elif self.strategy == ChunkingStrategy.TITLE_BASED:
            return await self._chunk_by_title(content, metadata)
        elif self.strategy == ChunkingStrategy.HYBRID:
            return await self._chunk_hybrid(content, metadata)
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–∏–Ω–≥–∞: {self.strategy}")

    async def _chunk_semantic(
        self,
        content: str,
        metadata: dict[str, Any] | None
    ) -> list[dict[str, Any]]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫–∏–Ω–≥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º sentence-based –ø–æ–¥—Ö–æ–¥–∞."""

        logger.debug("–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫–∏–Ω–≥")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º sentence chunker –∏–∑ chonkie –∫–∞–∫ –æ—Å–Ω–æ–≤—É
        sentence_chunks = self.sentence_chunker.chunk(content)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –ª–æ–≥–∏–∫—É
        semantic_chunks = await self._apply_semantic_logic(sentence_chunks)

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = []
        for i, chunk_content in enumerate(semantic_chunks):
            if len(chunk_content.strip()) < self.min_chunk_size:
                continue

            chunk_metadata = {
                "chunk_id": str(uuid.uuid4()),
                "chunk_index": i,
                "total_chunks": len(semantic_chunks),
                "chunking_strategy": "semantic",
                "chunk_type": "semantic_sentence",
                "character_count": len(chunk_content),
                "estimated_tokens": self._estimate_tokens(chunk_content),
                **(metadata or {})
            }

            result.append({
                "content": chunk_content.strip(),
                "metadata": chunk_metadata
            })

        logger.info("–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω", chunks_count=len(result))
        return result

    async def _chunk_by_title(
        self,
        content: str,
        metadata: dict[str, Any] | None
    ) -> list[dict[str, Any]]:
        """–ß–∞–Ω–∫–∏–Ω–≥ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π."""

        logger.debug("–ò—Å–ø–æ–ª—å–∑—É–µ–º title-based —á–∞–Ω–∫–∏–Ω–≥")

        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π
        title_patterns = [
            r'^#{1,6}\s+.+$',  # Markdown –∑–∞–≥–æ–ª–æ–≤–∫–∏
            r'^.+\n={3,}$',    # –ü–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–µ =
            r'^.+\n-{3,}$',    # –ü–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–µ -
            r'^\d+\.\s+.+$',   # –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            r'^[–ê-–ØA-Z][^.!?]*:$',  # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å –¥–≤–æ–µ—Ç–æ—á–∏–µ–º
        ]

        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
        sections = self._split_by_titles(content, title_patterns)

        # –ï—Å–ª–∏ —Ä–∞–∑–¥–µ–ª–æ–≤ –º–∞–ª–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
        if len(sections) < 2:
            logger.debug("–ó–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º paragraph-based —Ä–∞–∑–±–∏–≤–∫—É")
            sections = self._split_by_paragraphs(content)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å–µ–∫—Ü–∏—é
        result = []
        for i, (title, section_content) in enumerate(sections):
            # –ï—Å–ª–∏ —Å–µ–∫—Ü–∏—è —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–±–∏–≤–∞–µ–º
            if len(section_content) > self.chunk_size:
                sub_chunks = await self._split_large_section(section_content)

                for j, sub_chunk in enumerate(sub_chunks):
                    chunk_metadata = {
                        "chunk_id": str(uuid.uuid4()),
                        "chunk_index": len(result),
                        "chunking_strategy": "title",
                        "chunk_type": "title_section",
                        "section_title": title,
                        "sub_chunk_index": j,
                        "character_count": len(sub_chunk),
                        "estimated_tokens": self._estimate_tokens(sub_chunk),
                        **(metadata or {})
                    }

                    result.append({
                        "content": sub_chunk.strip(),
                        "metadata": chunk_metadata
                    })
            else:
                chunk_metadata = {
                    "chunk_id": str(uuid.uuid4()),
                    "chunk_index": i,
                    "chunking_strategy": "title",
                    "chunk_type": "title_section",
                    "section_title": title,
                    "character_count": len(section_content),
                    "estimated_tokens": self._estimate_tokens(section_content),
                    **(metadata or {})
                }

                result.append({
                    "content": section_content.strip(),
                    "metadata": chunk_metadata
                })

        logger.info("Title-based —á–∞–Ω–∫–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω", chunks_count=len(result))
        return result

    async def _chunk_hybrid(
        self,
        content: str,
        metadata: dict[str, Any] | None
    ) -> list[dict[str, Any]]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ."""

        logger.debug("–ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥")

        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é —Ä–∞–∑–±–∏–≤–∫—É
        title_chunks = await self._chunk_by_title(content, metadata)

        # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–æ—Å—å –º–∞–ª–æ —á–∞–Ω–∫–æ–≤ –∏–ª–∏ –æ–Ω–∏ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ, –ø—Ä–∏–º–µ–Ω—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ä–∞–∑–±–∏–≤–∫—É
        final_chunks = []

        for chunk in title_chunks:
            chunk_content = chunk["content"]
            chunk_tokens = chunk["metadata"]["estimated_tokens"]

            if chunk_tokens > self.chunk_size * 1.5:  # –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π —á–∞–Ω–∫
                logger.debug("–†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–æ–π —á–∞–Ω–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏",
                           chunk_tokens=chunk_tokens)

                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ä–∞–∑–±–∏–≤–∫—É –∫ –±–æ–ª—å—à–æ–º—É —á–∞–Ω–∫—É
                semantic_chunks = await self._chunk_semantic(chunk_content, metadata)

                for i, sem_chunk in enumerate(semantic_chunks):
                    # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞
                    sem_chunk["metadata"]["chunking_strategy"] = "hybrid"
                    sem_chunk["metadata"]["chunk_type"] = "hybrid_semantic"
                    sem_chunk["metadata"]["parent_title"] = chunk["metadata"].get("section_title", "")
                    sem_chunk["metadata"]["hybrid_index"] = i

                    final_chunks.append(sem_chunk)
            else:
                # –ß–∞–Ω–∫ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                chunk["metadata"]["chunking_strategy"] = "hybrid"
                chunk["metadata"]["chunk_type"] = "hybrid_title"
                final_chunks.append(chunk)

        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã
        for i, chunk in enumerate(final_chunks):
            chunk["metadata"]["chunk_index"] = i
            chunk["metadata"]["total_chunks"] = len(final_chunks)

        logger.info("–ì–∏–±—Ä–∏–¥–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω", chunks_count=len(final_chunks))
        return final_chunks

    async def _apply_semantic_logic(self, sentence_chunks: list[str]) -> list[str]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ª–æ–≥–∏–∫–∏ –∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º."""

        # –ü—Ä–æ—Å—Ç–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ª–æ–≥–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        semantic_chunks = []
        current_chunk = ""

        for sentence in sentence_chunks:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–Ω–æ –ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫ —Ç–µ–∫—É—â–µ–º—É —á–∞–Ω–∫—É
            potential_chunk = current_chunk + " " + sentence if current_chunk else sentence

            if len(potential_chunk) <= self.chunk_size:
                current_chunk = potential_chunk
            else:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫ –∏ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π
                if current_chunk:
                    semantic_chunks.append(current_chunk)
                current_chunk = sentence

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            semantic_chunks.append(current_chunk)

        return semantic_chunks

    def _split_by_titles(self, content: str, patterns: list[str]) -> list[tuple[str, str]]:
        """–†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º."""

        lines = content.split('\n')
        sections = []
        current_title = "–í–≤–µ–¥–µ–Ω–∏–µ"
        current_content = ""

        for line in lines:
            is_title = False

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –∑–∞–≥–æ–ª–æ–≤–∫–∞
            for pattern in patterns:
                if re.match(pattern, line.strip(), re.MULTILINE):
                    is_title = True
                    break

            if is_title and current_content.strip():
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å–µ–∫—Ü–∏—é
                sections.append((current_title, current_content.strip()))
                current_title = line.strip()
                current_content = ""
            else:
                current_content += line + "\n"

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–µ–∫—Ü–∏—é
        if current_content.strip():
            sections.append((current_title, current_content.strip()))

        return sections

    def _split_by_paragraphs(self, content: str) -> list[tuple[str, str]]:
        """–†–∞–∑–±–∏–≤–∫–∞ –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º –∫–∞–∫ fallback."""

        paragraphs = re.split(r'\n\s*\n', content)
        sections = []

        for i, paragraph in enumerate(paragraphs):
            if paragraph.strip():
                title = f"–ü–∞—Ä–∞–≥—Ä–∞—Ñ {i+1}"
                sections.append((title, paragraph.strip()))

        return sections

    async def _split_large_section(self, content: str) -> list[str]:
        """–†–∞–∑–±–∏–≤–∫–∞ –±–æ–ª—å—à–∏—Ö —Å–µ–∫—Ü–∏–π –Ω–∞ –º–µ–Ω—å—à–∏–µ —á–∞—Å—Ç–∏."""

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º recursive chunker –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏ –±–æ–ª—å—à–∏—Ö —Å–µ–∫—Ü–∏–π  
        chunks = self.recursive_chunker.chunk(content)

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏
        result = []
        for chunk in chunks:
            chunk_text = chunk.text if hasattr(chunk, 'text') else str(chunk)
            if len(chunk_text.strip()) >= self.min_chunk_size:
                result.append(chunk_text.strip())

        return result

    def _estimate_tokens(self, text: str) -> int:
        """–ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤."""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: ~4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ/–∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ
        return len(text) // 4

    # –ú–µ—Ç–æ–¥—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    async def chunk_text(
        self,
        text: str,
        metadata: dict[str, Any] | None = None
    ) -> list[dict[str, Any]]:
        """–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º API."""
        return await self.chunk_document(text, metadata)
