"""–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏."""

import re
import uuid
from datetime import datetime
from typing import Any

import numpy as np
import structlog
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

logger = structlog.get_logger(__name__)


class SemanticChunker:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ç–µ—Ö–Ω–∏–∫–∞–º–∏."""

    def __init__(
        self,
        embedding_model: SentenceTransformer,
        max_chunk_size: int = 1500,
        min_chunk_size: int = 100,
        similarity_threshold: float = 0.7,
        overlap_sentences: int = 2,
        window_size: int = 3,  # –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–≤—è–∑–Ω–æ—Å—Ç–∏
        adaptive_threshold: bool = True,  # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥
        hierarchical_chunking: bool = True  # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    ):
        self.embedding_model = embedding_model
        self.max_chunk_size = max_chunk_size
        self.min_chunk_size = min_chunk_size
        self.similarity_threshold = similarity_threshold
        self.overlap_sentences = overlap_sentences
        self.window_size = window_size
        self.adaptive_threshold = adaptive_threshold
        self.hierarchical_chunking = hierarchical_chunking

    def _protect_urls_and_emails(self, text: str) -> tuple[str, dict[str, str]]:
        """–≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞—Ç—å URL –∏ email –∞–¥—Ä–µ—Å–∞."""
        protected_text = text
        protected_entities = {}

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è URL –∏ email
        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+|www\.[^\s<>"{}|\\^`\[\]]+|\b[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}(?:/[^\s]*)?'
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'

        # –ù–∞—Ö–æ–¥–∏–º –∏ –∑–∞–º–µ–Ω—è–µ–º URL
        for match in re.finditer(url_pattern, text):
            url = match.group()
            placeholder = f"__URL_{uuid.uuid4().hex[:8]}__"
            protected_entities[placeholder] = url
            protected_text = protected_text.replace(url, placeholder, 1)

        # –ù–∞—Ö–æ–¥–∏–º –∏ –∑–∞–º–µ–Ω—è–µ–º email
        for match in re.finditer(email_pattern, text):
            email = match.group()
            placeholder = f"__EMAIL_{uuid.uuid4().hex[:8]}__"
            protected_entities[placeholder] = email
            protected_text = protected_text.replace(email, placeholder, 1)

        logger.debug("–ó–∞—â–∏—â–µ–Ω—ã —Å—É—â–Ω–æ—Å—Ç–∏",
                    urls_count=len([k for k in protected_entities.keys() if k.startswith("__URL_")]),
                    emails_count=len([k for k in protected_entities.keys() if k.startswith("__EMAIL_")]))

        return protected_text, protected_entities

    def _restore_protected_entities(self, text: str, protected_entities: dict[str, str]) -> str:
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ URL –∏ email."""
        for placeholder, original in protected_entities.items():
            text = text.replace(placeholder, original)
        return text

    def _split_into_sentences(self, text: str) -> list[str]:
        """–†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π."""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –±–µ–∑ lookbehind –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã
        sentences = []
        
        # –°–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∞–±–∑–∞—Ü–∞–º
        paragraphs = re.split(r'\n\s*\n', text)
        
        for paragraph in paragraphs:
            if not paragraph.strip():
                continue
                
            # –†–∞–∑–±–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π –∞–±–∑–∞—Ü –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º lookbehind
            sentence_pattern = r'(?<=[.!?])\s+(?=[A-Z–ê-–Ø])'
            para_sentences = re.split(sentence_pattern, paragraph)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–∞–º
            final_sentences = []
            for sent in para_sentences:
                sent = sent.strip()
                if not sent:
                    continue
                    
                # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å–ø–∏—Å–∫–∞–º –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
                if '\n' in sent:
                    # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø–µ—Ä–µ–Ω–æ—Å–∞–º —Å—Ç—Ä–æ–∫ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ —Å–ø–∏—Å–∫–æ–≤
                    lines = sent.split('\n')
                    current_sentence = ""
                    
                    for line in lines:
                        line = line.strip()
                        if not line:
                            if current_sentence:
                                final_sentences.append(current_sentence)
                                current_sentence = ""
                            continue
                            
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∏ (–Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ)
                        if (re.match(r'^[A-Z–ê-–Ø]', line) and 
                            len(line) < 100 and 
                            not line.endswith(('.', '!', '?'))):
                            if current_sentence:
                                final_sentences.append(current_sentence)
                                current_sentence = ""
                            final_sentences.append(line)
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–æ–≤
                        elif re.match(r'^[-‚Ä¢*]\s+', line) or re.match(r'^\d+\.\s+', line):
                            if current_sentence:
                                final_sentences.append(current_sentence)
                                current_sentence = ""
                            final_sentences.append(line)
                        else:
                            # –û–±—ã—á–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ - –¥–æ–±–∞–≤–ª—è–µ–º –∫ —Ç–µ–∫—É—â–µ–º—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é
                            if current_sentence:
                                current_sentence += " " + line
                            else:
                                current_sentence = line
                    
                    if current_sentence:
                        final_sentences.append(current_sentence)
                else:
                    final_sentences.append(sent)
            
            sentences.extend(final_sentences)

        # –û—á–∏—â–∞–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        cleaned_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and len(sentence) > 5:  # –£–º–µ–Ω—å—à–∏–ª –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
                cleaned_sentences.append(sentence)

        return cleaned_sentences

    def _calculate_semantic_similarity(self, sentences: list[str]) -> np.ndarray:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏."""
        if len(sentences) < 2:
            return np.array([[1.0]])

        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –±–∞—Ç—á–∞–º–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        batch_size = 32
        embeddings = []
        
        for i in range(0, len(sentences), batch_size):
            batch = sentences[i:i + batch_size]
            batch_embeddings = self.embedding_model.encode(
                batch, 
                convert_to_tensor=False,
                show_progress_bar=False
            )
            embeddings.extend(batch_embeddings)

        embeddings = np.array(embeddings)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarity_matrix = cosine_similarity(embeddings)

        return similarity_matrix

    def _calculate_adaptive_threshold(self, similarity_scores: list[float]) -> float:
        """–í—ã—á–∏—Å–ª–∏—Ç—å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞."""
        if not similarity_scores:
            return self.similarity_threshold
            
        scores_array = np.array(similarity_scores)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å + —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞
        percentile_25 = np.percentile(scores_array, 25)
        mean_score = np.mean(scores_array)
        std_score = np.std(scores_array)
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥: –º–µ–∂–¥—É 25-–º –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª–µ–º –∏ —Å—Ä–µ–¥–Ω–∏–º –º–∏–Ω—É—Å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
        adaptive_threshold = max(
            percentile_25,
            mean_score - 0.5 * std_score,
            0.3  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
        )
        
        logger.debug("–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥",
                    original_threshold=self.similarity_threshold,
                    adaptive_threshold=adaptive_threshold,
                    mean_similarity=mean_score,
                    std_similarity=std_score)
        
        return min(adaptive_threshold, self.similarity_threshold)

    def _find_semantic_boundaries_with_window(
        self, 
        sentences: list[str], 
        similarity_matrix: np.ndarray
    ) -> list[int]:
        """–ù–∞–π—Ç–∏ –≥—Ä–∞–Ω–∏—Ü—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞."""
        boundaries = [0]  # –ù–∞—á–∞–ª–æ –≤—Å–µ–≥–¥–∞ –≥—Ä–∞–Ω–∏—Ü–∞
        
        if len(sentences) < 2:
            boundaries.append(len(sentences))
            return boundaries

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞
        similarity_scores = []
        for i in range(1, len(sentences)):
            if i < len(similarity_matrix) and i-1 < len(similarity_matrix):
                similarity_scores.append(similarity_matrix[i-1, i])

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä–æ–≥
        threshold = (
            self._calculate_adaptive_threshold(similarity_scores) 
            if self.adaptive_threshold 
            else self.similarity_threshold
        )

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å –ø–æ–º–æ—â—å—é —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞
        for i in range(self.window_size, len(sentences) - self.window_size):
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–≤—è–∑–Ω–æ—Å—Ç—å –≤ –æ–∫–Ω–µ
            window_similarities = []
            
            # –°—Ö–æ–¥—Å—Ç–≤–æ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ–∫–Ω–∞
            for j in range(max(0, i - self.window_size), min(len(sentences), i + self.window_size + 1)):
                if j != i and j < len(similarity_matrix) and i < len(similarity_matrix[j]):
                    window_similarities.append(similarity_matrix[j, i])
            
            if not window_similarities:
                continue
                
            avg_window_similarity = np.mean(window_similarities)
            
            # –°—Ö–æ–¥—Å—Ç–≤–æ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º
            prev_similarity = similarity_matrix[i-1, i] if i > 0 else 1.0
            
            # –ö—Ä–∏—Ç–µ—Ä–∏–π —Ä–∞–∑—Ä—ã–≤–∞: –Ω–∏–∑–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º –ò –Ω–∏–∑–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –≤ –æ–∫–Ω–µ
            if (prev_similarity < threshold and 
                avg_window_similarity < threshold * 1.1):  # –ù–µ–º–Ω–æ–≥–æ –±–æ–ª–µ–µ –º—è–≥–∫–∏–π –∫—Ä–∏—Ç–µ—Ä–∏–π –¥–ª—è –æ–∫–Ω–∞
                
                boundaries.append(i)
                logger.debug("–ù–∞–π–¥–µ–Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –≥—Ä–∞–Ω–∏—Ü–∞ (–æ–∫–Ω–æ)",
                           position=i,
                           prev_similarity=prev_similarity,
                           window_similarity=avg_window_similarity,
                           threshold=threshold,
                           sentence_preview=sentences[i][:100])

        boundaries.append(len(sentences))  # –ö–æ–Ω–µ—Ü –≤—Å–µ–≥–¥–∞ –≥—Ä–∞–Ω–∏—Ü–∞
        return boundaries

    def _hierarchical_split(self, text: str) -> list[str]:
        """–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –¥–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        if len(text) <= self.max_chunk_size * 3:  # –ù–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
            return [text]
            
        # –°–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∫—Ä—É–ø–Ω—ã–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–∞–º
        major_delimiters = [
            r'\n#{1,3}\s+.+\n',  # –ó–∞–≥–æ–ª–æ–≤–∫–∏
            r'\n\n={3,}\n',      # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
            r'\n\n-{3,}\n',      # –î—Ä—É–≥–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
            r'\n\n\*{3,}\n',     # –ó–≤–µ–∑–¥–æ—á–∫–∏
        ]
        
        parts = [text]
        for delimiter in major_delimiters:
            new_parts = []
            for part in parts:
                split_parts = re.split(delimiter, part)
                new_parts.extend([p for p in split_parts if p.strip()])
            parts = new_parts
            
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞—Å—Ç–∏
        filtered_parts = []
        current_part = ""
        
        for part in parts:
            if len(current_part) + len(part) <= self.max_chunk_size * 2:
                current_part = (current_part + "\n\n" + part).strip()
            else:
                if current_part:
                    filtered_parts.append(current_part)
                current_part = part
                
        if current_part:
            filtered_parts.append(current_part)
            
        logger.debug("–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–±–∏–≤–∫–∞",
                    original_length=len(text),
                    parts_count=len(filtered_parts),
                    avg_part_size=sum(len(p) for p in filtered_parts) // len(filtered_parts) if filtered_parts else 0)
        
        return filtered_parts

    def _merge_small_chunks(self, chunks: list[str]) -> list[str]:
        """–û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π."""
        if not chunks:
            return []
            
        merged_chunks = []
        current_chunk = ""

        for chunk in chunks:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–µ–º –ª–∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å
            combined_length = len(current_chunk) + len(chunk)
            
            if combined_length <= self.max_chunk_size:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —É–º–Ω—ã–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º
                separator = "\n\n" if current_chunk and not current_chunk.endswith('\n') else " "
                current_chunk = (current_chunk + separator + chunk).strip()
            else:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫ –µ—Å–ª–∏ –æ–Ω –Ω–µ –ø—É—Å—Ç–æ–π
                if current_chunk:
                    merged_chunks.append(current_chunk)
                current_chunk = chunk

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            merged_chunks.append(current_chunk)

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –æ–±—ä–µ–¥–∏–Ω—è–µ–º –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —á–∞–Ω–∫–∏
        if (len(merged_chunks) > 1 and 
            len(merged_chunks[-1]) < self.min_chunk_size and
            len(merged_chunks[-2]) + len(merged_chunks[-1]) <= self.max_chunk_size):
            
            last_chunk = merged_chunks.pop()
            merged_chunks[-1] = merged_chunks[-1] + "\n\n" + last_chunk
            
        return merged_chunks

    def _split_large_chunks(self, chunks: list[str]) -> list[str]:
        """–†–∞–∑–¥–µ–ª–∏—Ç—å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏–∫–∏."""
        final_chunks = []

        for chunk in chunks:
            if len(chunk) <= self.max_chunk_size:
                final_chunks.append(chunk)
                continue
                
            # –î–ª—è –±–æ–ª—å—à–∏—Ö —á–∞–Ω–∫–æ–≤ –ø—Ä–∏–º–µ–Ω—è–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—É—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ä–∞–∑–±–∏–≤–∫—É
            logger.debug("–†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–æ–π —á–∞–Ω–∫", size=len(chunk))
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –∏—â–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã
            sentences = self._split_into_sentences(chunk)
            if len(sentences) <= 1:
                # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º —Ä–∞–∑–±–∏—Ç—å –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –¥–µ–ª–∞–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
                words = chunk.split()
                chunk_size_words = self.max_chunk_size // 7  # –ü—Ä–∏–º–µ—Ä–Ω–æ 7 —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ —Å–ª–æ–≤–æ
                
                for i in range(0, len(words), chunk_size_words):
                    word_chunk = " ".join(words[i:i + chunk_size_words])
                    final_chunks.append(word_chunk)
            else:
                # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
                similarity_matrix = self._calculate_semantic_similarity(sentences)
                boundaries = self._find_semantic_boundaries_with_window(sentences, similarity_matrix)
                
                # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
                for i in range(len(boundaries) - 1):
                    start_idx = boundaries[i]
                    end_idx = boundaries[i + 1]
                    sentence_chunk = " ".join(sentences[start_idx:end_idx])
                    
                    # –ï—Å–ª–∏ —á–∞–Ω–∫ –≤—Å–µ –µ—â–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ
                    if len(sentence_chunk) > self.max_chunk_size:
                        sub_sentences = sentences[start_idx:end_idx]
                        current_sub_chunk = ""
                        
                        for sentence in sub_sentences:
                            if len(current_sub_chunk) + len(sentence) <= self.max_chunk_size:
                                current_sub_chunk = (current_sub_chunk + " " + sentence).strip()
                            else:
                                if current_sub_chunk:
                                    final_chunks.append(current_sub_chunk)
                                current_sub_chunk = sentence
                                
                        if current_sub_chunk:
                            final_chunks.append(current_sub_chunk)
                    else:
                        final_chunks.append(sentence_chunk)

        return final_chunks

    def _add_semantic_overlap(self, chunks: list[str], sentences: list[str]) -> list[str]:
        """–î–æ–±–∞–≤–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏."""
        if len(chunks) <= 1 or self.overlap_sentences == 0:
            return chunks

        overlapped_chunks = []
        sentence_to_chunk = {}  # –ú–∞–ø–ø–∏–Ω–≥ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∫ —á–∞–Ω–∫–∞–º
        
        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∫ —á–∞–Ω–∫–∞–º
        sentence_idx = 0
        for chunk_idx, chunk in enumerate(chunks):
            chunk_sentences = self._split_into_sentences(chunk)
            for sent in chunk_sentences:
                if sentence_idx < len(sentences):
                    sentence_to_chunk[sentence_idx] = chunk_idx
                    sentence_idx += 1

        for i, chunk in enumerate(chunks):
            if i == 0:
                # –ü–µ—Ä–≤—ã–π —á–∞–Ω–∫ –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
                overlapped_chunks.append(chunk)
            else:
                # –ò—â–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —á–∞–Ω–∫–∞
                prev_chunk_sentences = self._split_into_sentences(chunks[i-1])
                
                # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
                overlap_count = min(self.overlap_sentences, len(prev_chunk_sentences))
                overlap_sentences = prev_chunk_sentences[-overlap_count:] if overlap_count > 0 else []
                
                if overlap_sentences:
                    overlap_text = " ".join(overlap_sentences)
                    # –£–º–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                    separator = "\n\n" if chunk.startswith(chunk.split()[0].upper()) else " "
                    overlapped_chunk = overlap_text + separator + chunk
                    overlapped_chunks.append(overlapped_chunk)
                else:
                    overlapped_chunks.append(chunk)

        return overlapped_chunks

    def _analyze_chunk_quality(self, chunks: list[str]) -> dict[str, Any]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–±–∏–≤–∫–∏ –Ω–∞ —á–∞–Ω–∫–∏."""
        if not chunks:
            return {"error": "–ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"}

        sizes = [len(chunk) for chunk in chunks]
        word_counts = [len(chunk.split()) for chunk in chunks]
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_chunks = len(chunks)
        avg_size = sum(sizes) / len(sizes) if sizes else 0
        min_size = min(sizes) if sizes else 0
        max_size = max(sizes) if sizes else 0
        avg_words = sum(word_counts) / len(word_counts) if word_counts else 0

        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        small_chunks = [s for s in sizes if s < self.min_chunk_size]
        large_chunks = [s for s in sizes if s > self.max_chunk_size * 1.2]
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤
        size_std = np.std(sizes) if len(sizes) > 1 else 0
        size_variance = np.var(sizes) if len(sizes) > 1 else 0
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ (0-100)
        quality_score = 100
        
        # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –ø—Ä–æ–±–ª–µ–º—ã
        if small_chunks:
            quality_score -= len(small_chunks) * 10
        if large_chunks:
            quality_score -= len(large_chunks) * 15
        if size_std > avg_size * 0.5:  # –í—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ä–æ–≤
            quality_score -= 20
            
        quality_score = max(0, quality_score)

        analysis = {
            "total_chunks": total_chunks,
            "size_stats": {
                "average": round(avg_size, 1),
                "min": min_size,
                "max": max_size,
                "std_dev": round(size_std, 1),
                "variance": round(size_variance, 1)
            },
            "word_stats": {
                "average_words": round(avg_words, 1),
                "min_words": min(word_counts) if word_counts else 0,
                "max_words": max(word_counts) if word_counts else 0
            },
            "quality_metrics": {
                "score": round(quality_score, 1),
                "small_chunks_count": len(small_chunks),
                "large_chunks_count": len(large_chunks),
                "size_consistency": "good" if size_std < avg_size * 0.3 else "needs_improvement"
            },
            "distribution": {
                "size_quartiles": {
                    "q1": round(np.percentile(sizes, 25), 1) if sizes else 0,
                    "q2": round(np.percentile(sizes, 50), 1) if sizes else 0,
                    "q3": round(np.percentile(sizes, 75), 1) if sizes else 0
                }
            }
        }

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = []
        if len(small_chunks) > total_chunks * 0.2:
            recommendations.append("–£–≤–µ–ª–∏—á—å—Ç–µ min_chunk_size –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç–µ similarity_threshold")
        if len(large_chunks) > total_chunks * 0.1:
            recommendations.append("–£–º–µ–Ω—å—à–∏—Ç–µ max_chunk_size –∏–ª–∏ –∞–∫—Ç–∏–≤–∏—Ä—É–π—Ç–µ hierarchical_chunking")
        if size_std > avg_size * 0.5:
            recommendations.append("–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –±–æ–ª–µ–µ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–π —Ä–∞–∑–±–∏–≤–∫–∏")
            
        analysis["recommendations"] = recommendations

        return analysis

    def _extract_key_terms(self, text: str, max_terms: int = 10) -> list[str]:
        """–ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        clean_text = re.sub(r'[^\w\s]', ' ', text.lower())
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–æ–π –æ—Ç 4 —Å–∏–º–≤–æ–ª–æ–≤
        words = re.findall(r'\b\w{4,}\b', clean_text)
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ –∏ –±–µ—Ä–µ–º —Ç–æ–ø —Ç–µ—Ä–º–∏–Ω–æ–≤
        sorted_terms = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        key_terms = [term for term, freq in sorted_terms[:max_terms] if freq > 1]
        
        return key_terms

    def _create_enhanced_metadata(self, content: str, base_metadata: dict[str, Any] | None) -> dict[str, Any]:
        """–°–æ–∑–¥–∞—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —á–∞–Ω–∫–∞."""
        metadata = (base_metadata or {}).copy()
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        metadata.update({
            "content_length": len(content),
            "word_count": len(content.split()),
            "sentence_count": len(self._split_into_sentences(content)),
            "processing_timestamp": datetime.now().isoformat(),
        })
        
        # –ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        key_terms = self._extract_key_terms(content)
        metadata["key_terms"] = key_terms
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_lower = content.lower()
        content_features = {
            "has_urls": bool(re.search(r'https?://', content)),
            "has_emails": bool(re.search(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', content)),
            "has_code": bool(re.search(r'[{}()[\]<>]', content)) and content.count('{') > 2,
            "has_numbers": bool(re.search(r'\d+', content)),
            "is_list_like": content.count('\n-') > 2 or content.count('‚Ä¢') > 2,
        }
        metadata["content_features"] = content_features
        
        # –Ø–∑—ã–∫–æ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
        russian_chars = len(re.findall(r'[–∞-—è—ë]', content_lower))
        english_chars = len(re.findall(r'[a-z]', content_lower))
        total_letters = russian_chars + english_chars
        
        if total_letters > 0:
            metadata["language_mix"] = {
                "russian_ratio": round(russian_chars / total_letters, 2),
                "english_ratio": round(english_chars / total_letters, 2),
                "primary_language": "russian" if russian_chars > english_chars else "english"
            }
        
        return metadata

    def _create_contextual_text(self, content: str, metadata: dict[str, Any]) -> str:
        """–°–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞."""
        contextual_parts = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞/—Å—Ç—Ä–∞–Ω–∏—Ü—ã
        title = metadata.get("title")
        if title:
            contextual_parts.append(f"page_title: {title}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π URL –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–±–µ–∑ –æ–±—Ä–µ–∑–∞–Ω–∏—è)
        source = metadata.get("source")
        if source:
            contextual_parts.append(f"page_url: {source}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é/—Ç–∏–ø
        category = metadata.get("category") or metadata.get("type")
        if category:
            contextual_parts.append(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª/–≥–ª–∞–≤—É
        section = metadata.get("section") or metadata.get("chapter")
        if section:
            contextual_parts.append(f"–†–∞–∑–¥–µ–ª: {section}")
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
        if contextual_parts:
            context_header = " | ".join(contextual_parts)
            return f"{context_header}\n\n{content}"
        
        return content

    async def chunk_text(
        self,
        text: str,
        metadata: dict[str, Any] | None = None
    ) -> list[dict[str, Any]]:
        """–†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏ —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ç–µ—Ö–Ω–∏–∫–∞–º–∏."""
        if not text or len(text.strip()) < self.min_chunk_size:
            logger.warning("–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏", length=len(text) if text else 0)
            return []

        logger.info("üß† –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ä–∞–∑–±–∏–≤–∫—É",
                   text_length=len(text),
                   similarity_threshold=self.similarity_threshold,
                   adaptive_threshold=self.adaptive_threshold,
                   hierarchical_chunking=self.hierarchical_chunking,
                   window_size=self.window_size)

        start_time = datetime.now()

        # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –¥–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if self.hierarchical_chunking and len(text) > self.max_chunk_size * 3:
            logger.info("–ü—Ä–∏–º–µ–Ω—è–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Ä–∞–∑–±–∏–≤–∫—É –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
            text_parts = self._hierarchical_split(text)
            
            all_result_chunks = []
            for part_idx, part in enumerate(text_parts):
                logger.debug(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç—å {part_idx + 1}/{len(text_parts)}", size=len(part))
                
                # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å –ë–ï–ó –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Ä–∞–∑–±–∏–≤–∫–∏
                # —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–π —Ä–µ–∫—É—Ä—Å–∏–∏
                part_chunker = SemanticChunker(
                    embedding_model=self.embedding_model,
                    max_chunk_size=self.max_chunk_size,
                    min_chunk_size=self.min_chunk_size,
                    similarity_threshold=self.similarity_threshold,
                    overlap_sentences=self.overlap_sentences,
                    window_size=self.window_size,
                    adaptive_threshold=self.adaptive_threshold,
                    hierarchical_chunking=False  # –û–¢–ö–õ–Æ–ß–ê–ï–ú –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —Ä–µ–∫—É—Ä—Å–∏–∏
                )
                part_chunks = await part_chunker.chunk_text(part, metadata)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —á–∞—Å—Ç–µ–π
                for chunk in part_chunks:
                    chunk["metadata"].update({
                        "hierarchical_part": part_idx,
                        "total_hierarchical_parts": len(text_parts)
                    })
                
                all_result_chunks.extend(part_chunks)
            
            logger.info("‚úÖ –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞", 
                       total_chunks=len(all_result_chunks),
                       hierarchical_parts=len(text_parts))
            return all_result_chunks

        # –ó–∞—â–∏—â–∞–µ–º URL –∏ email
        protected_text, protected_entities = self._protect_urls_and_emails(text)
        logger.debug("–ó–∞—â–∏—â–µ–Ω—ã —Å—É—â–Ω–æ—Å—Ç–∏", entities_count=len(protected_entities))

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = self._split_into_sentences(protected_text)
        logger.debug("–†–∞–∑–±–∏—Ç–æ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è", sentences_count=len(sentences))

        if len(sentences) <= 1:
            # –ï—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –º–∞–ª–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞–∫ –æ–¥–∏–Ω —á–∞–Ω–∫
            logger.info("–ú–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –æ–¥–∏–Ω —á–∞–Ω–∫")
            restored_text = self._restore_protected_entities(text, protected_entities)
            enhanced_metadata = self._create_enhanced_metadata(restored_text, metadata)
            contextual_text = self._create_contextual_text(restored_text, enhanced_metadata)
            
            return [{
                "content": contextual_text,
                "metadata": enhanced_metadata
            }]

        # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarity_matrix = self._calculate_semantic_similarity(sentences)
        logger.debug("–°–æ–∑–¥–∞–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞", shape=similarity_matrix.shape)

        # –ù–∞—Ö–æ–¥–∏–º –≥—Ä–∞–Ω–∏—Ü—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤ —Å –æ–∫–Ω–æ–º
        boundaries = self._find_semantic_boundaries_with_window(sentences, similarity_matrix)
        logger.debug("–ù–∞–π–¥–µ–Ω—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã", boundaries=boundaries, 
                    boundaries_count=len(boundaries)-2)  # -2 —Ç–∞–∫ –∫–∞–∫ –ø–µ—Ä–≤–∞—è –∏ –ø–æ—Å–ª–µ–¥–Ω—è—è - —ç—Ç–æ –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü

        # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        chunks = []
        for i in range(len(boundaries) - 1):
            start_idx = boundaries[i]
            end_idx = boundaries[i + 1]
            chunk_sentences = sentences[start_idx:end_idx]
            chunk_text = " ".join(chunk_sentences)
            chunks.append(chunk_text)

        logger.debug("–°–æ–∑–¥–∞–Ω—ã –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏", count=len(chunks))

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏
        chunks = self._merge_small_chunks(chunks)
        logger.debug("–ü–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–∞–ª–µ–Ω—å–∫–∏—Ö", count=len(chunks))

        # –†–∞–∑–¥–µ–ª—è–µ–º –±–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏
        chunks = self._split_large_chunks(chunks)
        logger.debug("–ü–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö", count=len(chunks))

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
        chunks = self._add_semantic_overlap(chunks, sentences)
        logger.debug("–ü–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è", count=len(chunks))

        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞—â–∏—â–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        chunks = [self._restore_protected_entities(chunk, protected_entities) for chunk in chunks]

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        quality_analysis = self._analyze_chunk_quality(chunks)
        
        processing_time = (datetime.now() - start_time).total_seconds()

        logger.info("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞",
                   chunks_created=len(chunks),
                   processing_time_seconds=round(processing_time, 2),
                   quality_score=quality_analysis.get("quality_metrics", {}).get("score", 0),
                   avg_chunk_size=quality_analysis.get("size_stats", {}).get("average", 0),
                   size_consistency=quality_analysis.get("quality_metrics", {}).get("size_consistency", "unknown"))

        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        recommendations = quality_analysis.get("recommendations", [])
        if recommendations:
            logger.warning("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–∏–Ω–≥–∞", recommendations=recommendations)

        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        result_chunks = []
        for i, chunk in enumerate(chunks):
            # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            enhanced_metadata = self._create_enhanced_metadata(chunk, metadata)
            enhanced_metadata.update({
                "chunk_index": i,
                "total_chunks": len(chunks),
                "chunk_type": "semantic_advanced",
                "chunking_config": {
                    "similarity_threshold": self.similarity_threshold,
                    "adaptive_threshold": self.adaptive_threshold,
                    "window_size": self.window_size,
                    "hierarchical_chunking": self.hierarchical_chunking,
                    "overlap_sentences": self.overlap_sentences
                },
                "processing_time": processing_time,
                "quality_analysis": quality_analysis if i == 0 else None,  # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑ —Ç–æ–ª—å–∫–æ –∫ –ø–µ—Ä–≤–æ–º—É —á–∞–Ω–∫—É
            })

            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
            contextual_text = self._create_contextual_text(chunk, enhanced_metadata)
            
            result_chunks.append({
                "content": contextual_text,
                "metadata": enhanced_metadata
            })

        return result_chunks
