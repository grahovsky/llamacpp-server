"""–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏."""

import re
import uuid
from typing import List, Dict, Any, Tuple, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import structlog
from sklearn.metrics.pairwise import cosine_similarity

logger = structlog.get_logger(__name__)


class SemanticChunker:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏."""
    
    def __init__(
        self,
        embedding_model: SentenceTransformer,
        max_chunk_size: int = 1500,
        min_chunk_size: int = 100,
        similarity_threshold: float = 0.7,
        overlap_sentences: int = 2
    ):
        self.embedding_model = embedding_model
        self.max_chunk_size = max_chunk_size
        self.min_chunk_size = min_chunk_size
        self.similarity_threshold = similarity_threshold
        self.overlap_sentences = overlap_sentences
        
    def _protect_urls_and_emails(self, text: str) -> Tuple[str, Dict[str, str]]:
        """–≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞—Ç—å URL –∏ email –∞–¥—Ä–µ—Å–∞."""
        protected_text = text
        protected_entities = {}
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è URL –∏ email
        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+|www\.[^\s<>"{}|\\^`\[\]]+|\b[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}(?:/[^\s]*)?'
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        
        # –ù–∞—Ö–æ–¥–∏–º –∏ –∑–∞–º–µ–Ω—è–µ–º URL
        for match in re.finditer(url_pattern, text):
            url = match.group()
            placeholder = f"__URL_{uuid.uuid4().hex[:8]}__"
            protected_entities[placeholder] = url
            protected_text = protected_text.replace(url, placeholder, 1)
        
        # –ù–∞—Ö–æ–¥–∏–º –∏ –∑–∞–º–µ–Ω—è–µ–º email
        for match in re.finditer(email_pattern, text):
            email = match.group()
            placeholder = f"__EMAIL_{uuid.uuid4().hex[:8]}__"
            protected_entities[placeholder] = email
            protected_text = protected_text.replace(email, placeholder, 1)
            
        logger.debug("–ó–∞—â–∏—â–µ–Ω—ã —Å—É—â–Ω–æ—Å—Ç–∏", 
                    urls_count=len([k for k in protected_entities.keys() if k.startswith("__URL_")]),
                    emails_count=len([k for k in protected_entities.keys() if k.startswith("__EMAIL_")]))
        
        return protected_text, protected_entities
    
    def _restore_protected_entities(self, text: str, protected_entities: Dict[str, str]) -> str:
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ URL –∏ email."""
        for placeholder, original in protected_entities.items():
            text = text.replace(placeholder, original)
        return text
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."""
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentence_pattern = r'(?<=[.!?])\s+(?=[A-Z–ê-–Ø])|(?<=\n\n)|(?<=\n(?=[A-Z–ê-–Ø]))'
        sentences = re.split(sentence_pattern, text)
        
        # –û—á–∏—â–∞–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        cleaned_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and len(sentence) > 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                cleaned_sentences.append(sentence)
        
        return cleaned_sentences
    
    def _calculate_semantic_similarity(self, sentences: List[str]) -> np.ndarray:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏."""
        if len(sentences) < 2:
            return np.array([[1.0]])
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        embeddings = self.embedding_model.encode(sentences, convert_to_tensor=False)
        embeddings = np.array(embeddings)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarity_matrix = cosine_similarity(embeddings)
        
        return similarity_matrix
    
    def _find_semantic_boundaries(self, sentences: List[str], similarity_matrix: np.ndarray) -> List[int]:
        """–ù–∞–π—Ç–∏ –≥—Ä–∞–Ω–∏—Ü—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤."""
        boundaries = [0]  # –ù–∞—á–∞–ª–æ –≤—Å–µ–≥–¥–∞ –≥—Ä–∞–Ω–∏—Ü–∞
        
        for i in range(1, len(sentences)):
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–µ–∫—É—â–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º
            prev_similarity = similarity_matrix[i-1, i] if i > 0 else 1.0
            
            # –ï—Å–ª–∏ —Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞ - —ç—Ç–æ –≥—Ä–∞–Ω–∏—Ü–∞ –Ω–æ–≤–æ–≥–æ –±–ª–æ–∫–∞
            if prev_similarity < self.similarity_threshold:
                boundaries.append(i)
                logger.debug("–ù–∞–π–¥–µ–Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –≥—Ä–∞–Ω–∏—Ü–∞", 
                           position=i, 
                           similarity=prev_similarity,
                           sentence_preview=sentences[i][:100])
        
        boundaries.append(len(sentences))  # –ö–æ–Ω–µ—Ü –≤—Å–µ–≥–¥–∞ –≥—Ä–∞–Ω–∏—Ü–∞
        return boundaries
    
    def _merge_small_chunks(self, chunks: List[str]) -> List[str]:
        """–û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏."""
        merged_chunks = []
        current_chunk = ""
        
        for chunk in chunks:
            if len(current_chunk) + len(chunk) <= self.max_chunk_size:
                current_chunk = (current_chunk + " " + chunk).strip()
            else:
                if current_chunk:
                    merged_chunks.append(current_chunk)
                current_chunk = chunk
        
        if current_chunk:
            merged_chunks.append(current_chunk)
        
        return merged_chunks
    
    def _split_large_chunks(self, chunks: List[str]) -> List[str]:
        """–†–∞–∑–¥–µ–ª–∏—Ç—å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏."""
        final_chunks = []
        
        for chunk in chunks:
            if len(chunk) <= self.max_chunk_size:
                final_chunks.append(chunk)
            else:
                # –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–æ–π —á–∞–Ω–∫ –Ω–∞ —á–∞—Å—Ç–∏
                sentences = self._split_into_sentences(chunk)
                current_chunk = ""
                
                for sentence in sentences:
                    if len(current_chunk) + len(sentence) <= self.max_chunk_size:
                        current_chunk = (current_chunk + " " + sentence).strip()
                    else:
                        if current_chunk:
                            final_chunks.append(current_chunk)
                        current_chunk = sentence
                
                if current_chunk:
                    final_chunks.append(current_chunk)
        
        return final_chunks
    
    def _add_sentence_overlap(self, chunks: List[str], sentences: List[str]) -> List[str]:
        """–î–æ–±–∞–≤–∏—Ç—å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏."""
        if len(chunks) <= 1 or self.overlap_sentences == 0:
            return chunks
        
        overlapped_chunks = []
        
        for i, chunk in enumerate(chunks):
            if i == 0:
                # –ü–µ—Ä–≤—ã–π —á–∞–Ω–∫ –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
                overlapped_chunks.append(chunk)
            else:
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —á–∞–Ω–∫–∞
                prev_chunk = chunks[i-1]
                prev_sentences = self._split_into_sentences(prev_chunk)
                
                # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
                overlap_sentences = prev_sentences[-self.overlap_sentences:] if len(prev_sentences) >= self.overlap_sentences else prev_sentences
                overlap_text = " ".join(overlap_sentences)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –∫ —Ç–µ–∫—É—â–µ–º—É —á–∞–Ω–∫—É
                overlapped_chunk = overlap_text + " " + chunk
                overlapped_chunks.append(overlapped_chunk)
        
        return overlapped_chunks
    
    async def chunk_text(
        self, 
        text: str, 
        metadata: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """–†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏."""
        logger.info("üß† –ù–∞—á–∏–Ω–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ä–∞–∑–±–∏–≤–∫—É —Ç–µ–∫—Å—Ç–∞", 
                   text_length=len(text),
                   max_chunk_size=self.max_chunk_size)
        
        if not text or len(text) < self.min_chunk_size:
            logger.warning("–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏", length=len(text))
            return []
        
        # –ó–∞—â–∏—â–∞–µ–º URL –∏ email
        protected_text, protected_entities = self._protect_urls_and_emails(text)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = self._split_into_sentences(protected_text)
        logger.debug("–¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è", count=len(sentences))
        
        if len(sentences) < 2:
            # –ï—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –º–∞–ª–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –æ–¥–∏–Ω —á–∞–Ω–∫
            restored_text = self._restore_protected_entities(protected_text, protected_entities)
            return [{
                "content": restored_text,
                "metadata": metadata or {},
                "chunk_type": "single"
            }]
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarity_matrix = self._calculate_semantic_similarity(sentences)
        
        # –ù–∞—Ö–æ–¥–∏–º –≥—Ä–∞–Ω–∏—Ü—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤
        boundaries = self._find_semantic_boundaries(sentences, similarity_matrix)
        logger.debug("–ù–∞–π–¥–µ–Ω—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã", boundaries=boundaries)
        
        # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º
        chunks = []
        for i in range(len(boundaries) - 1):
            start = boundaries[i]
            end = boundaries[i + 1]
            chunk_sentences = sentences[start:end]
            chunk_text = " ".join(chunk_sentences)
            chunks.append(chunk_text)
        
        logger.debug("–°–æ–∑–¥–∞–Ω—ã –ø–µ—Ä–≤–∏—á–Ω—ã–µ —á–∞–Ω–∫–∏", count=len(chunks))
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏
        chunks = self._merge_small_chunks(chunks)
        logger.debug("–ü–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–∞–ª–µ–Ω—å–∫–∏—Ö —á–∞–Ω–∫–æ–≤", count=len(chunks))
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –±–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏
        chunks = self._split_large_chunks(chunks)
        logger.debug("–ü–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —á–∞–Ω–∫–æ–≤", count=len(chunks))
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        chunks = self._add_sentence_overlap(chunks, sentences)
        logger.debug("–ü–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è", count=len(chunks))
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞—â–∏—â–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏ —Å–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏
        final_chunks = []
        for i, chunk in enumerate(chunks):
            restored_chunk = self._restore_protected_entities(chunk, protected_entities)
            
            chunk_metadata = (metadata or {}).copy()
            chunk_metadata.update({
                "chunk_index": i,
                "total_chunks": len(chunks),
                "chunk_type": "semantic",
                "chunk_size": len(restored_chunk)
            })
            
            final_chunks.append({
                "content": restored_chunk,
                "metadata": chunk_metadata
            })
        
        logger.info("‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞", 
                   original_length=len(text),
                   chunks_count=len(final_chunks),
                   avg_chunk_size=sum(len(chunk["content"]) for chunk in final_chunks) // len(final_chunks))
        
        return final_chunks 