"""–†–æ—É—Ç–µ—Ä –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏."""

import os
from typing import Dict, Any, Optional

import structlog
from fastapi import APIRouter, Depends, HTTPException, Request, BackgroundTasks
from pydantic import BaseModel

from ...config import get_settings
from ...domain.protocols import LlamaServiceProtocol

router = APIRouter()
logger = structlog.get_logger(__name__)


class GenerationParams(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."""
    temperature: Optional[float] = None
    repeat_penalty: Optional[float] = None
    top_p: Optional[float] = None
    top_k: Optional[int] = None
    max_tokens: Optional[int] = None


class TestPrompt(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤."""
    prompt: str
    params: Optional[GenerationParams] = None
    stream: bool = False


class DebugInfo(BaseModel):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏."""
    message: str
    current_params: Dict[str, Any]
    model_info: Dict[str, Any]
    rag_status: Dict[str, Any]


async def get_llama_service(request: Request) -> LlamaServiceProtocol:
    """–ü–æ–ª—É—á–∏—Ç—å LLama —Å–µ—Ä–≤–∏—Å –∏–∑ DI –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞.""" 
    return request.app.state.container.llama_service()


@router.get("/debug/info", response_model=DebugInfo)
async def get_debug_info(
    llama_service: LlamaServiceProtocol = Depends(get_llama_service)
):
    """–ü–æ–ª—É—á–∏—Ç—å –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏—Å—Ç–µ–º–µ."""
    settings = get_settings()
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
    model_info = {
        "model_path": str(settings.model_path),
        "model_exists": settings.model_path.exists() if settings.model_path else False,
        "model_size": os.path.getsize(settings.model_path) if settings.model_path and settings.model_path.exists() else 0,
        "n_ctx": settings.n_ctx,
        "n_gpu_layers": settings.n_gpu_layers,
        "ready": await llama_service.is_ready()
    }
    
    # –¢–µ–∫—É—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    current_params = {
        "temperature": settings.temperature,
        "repeat_penalty": settings.repeat_penalty,
        "top_p": settings.top_p,
        "top_k": settings.top_k,
        "max_response_tokens": settings.max_response_tokens,
        "max_history_tokens": settings.max_history_tokens
    }
    
    # –°—Ç–∞—Ç—É—Å RAG
    rag_status = {
        "enabled": settings.enable_rag,
        "data_dir": str(settings.data_dir),
        "chunk_size": settings.chunk_size,
        "chunk_overlap": settings.chunk_overlap,
        "max_retrieved_chunks": settings.max_retrieved_chunks
    }
    
    return DebugInfo(
        message="–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –æ—Ç–ª–∞–¥–∫–µ",
        current_params=current_params,
        model_info=model_info,
        rag_status=rag_status
    )


@router.post("/debug/test-prompt")
async def test_prompt(
    test_data: TestPrompt,
    llama_service: LlamaServiceProtocol = Depends(get_llama_service)
):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏."""
    try:
        from ...domain.models import ChatCompletionRequest, ChatMessage
        
        # –°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        messages = [ChatMessage(role="user", content=test_data.prompt)]
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        request = ChatCompletionRequest(
            messages=messages,
            model="test-model",
            stream=test_data.stream,
            temperature=test_data.params.temperature if test_data.params else None,
            repeat_penalty=test_data.params.repeat_penalty if test_data.params else None,
            top_p=test_data.params.top_p if test_data.params else None,
            top_k=test_data.params.top_k if test_data.params else None,
            max_tokens=test_data.params.max_tokens if test_data.params else None
        )
        
        logger.info("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞", 
                   prompt_preview=test_data.prompt[:100],
                   params=test_data.params.dict() if test_data.params else None)
        
        if test_data.stream:
            # –î–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
            async def stream_response():
                async for chunk in llama_service.chat_completion_stream(request):
                    yield chunk
            return stream_response()
        else:
            # –û–±—ã—á–Ω—ã–π –æ—Ç–≤–µ—Ç
            response = await llama_service.chat_completion(request)
            return response
            
    except Exception as e:
        logger.error("‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞", error=str(e), exc_info=True)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {str(e)}")


@router.post("/debug/analyze-loop")
async def analyze_loop_potential(
    test_data: TestPrompt
):
    """–ê–Ω–∞–ª–∏–∑ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞."""
    from ...llama.loop_detector import LoopDetector
    
    try:
        # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–µ–∫—Ç–æ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        detector = LoopDetector(max_repeats=2, min_repeat_length=5)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        words = test_data.prompt.split()
        chunk_size = 10
        chunks = [" ".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]
        
        potential_issues = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π chunk
        for i, chunk in enumerate(chunks):
            if detector.add_chunk(chunk):
                potential_issues.append(f"–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏–µ –≤ —á–∞—Å—Ç–∏ {i+1}: {chunk[:50]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã
        prompt_lower = test_data.prompt.lower()
        repeat_phrases = [
            "–ø–æ–∂–∞–ª—É–π—Å—Ç–∞",
            "–ø–æ–¥–æ–∂–¥–∏—Ç–µ", 
            "–ø—Ä–æ–≤–µ—Ä–∏—Ç—å",
            "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
            "–æ—Ç–≤–µ—Ç",
            "–≤–æ–ø—Ä–æ—Å"
        ]
        
        phrase_counts = {}
        for phrase in repeat_phrases:
            count = prompt_lower.count(phrase)
            if count > 1:
                phrase_counts[phrase] = count
        
        stats = detector.get_statistics()
        
        return {
            "prompt_analysis": {
                "length": len(test_data.prompt),
                "word_count": len(words),
                "chunks_analyzed": len(chunks)
            },
            "potential_issues": potential_issues,
            "repeated_phrases": phrase_counts,
            "detector_stats": stats,
            "recommendations": _get_loop_prevention_recommendations(test_data.prompt, phrase_counts)
        }
        
    except Exception as e:
        logger.error("‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è", error=str(e), exc_info=True)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")


@router.post("/debug/emergency-stop")
async def emergency_stop():
    """–≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π (–∑–∞–≥–ª—É—à–∫–∞)."""
    logger.warning("üö® –ó–∞–ø—Ä–æ—Å —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
    
    # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∞–∫—Ç–∏–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π
    # –ü–æ–∫–∞ —á—Ç–æ –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º
    
    return {
        "message": "–≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏–Ω–∏—Ü–∏–∏—Ä–æ–≤–∞–Ω–∞",
        "status": "acknowledged",
        "note": "–ú–µ—Ö–∞–Ω–∏–∑–º –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∞–∫—Ç–∏–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ"
    }


@router.get("/debug/generation-stats")
async def get_generation_stats():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–π."""
    # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    return {
        "total_requests": "unknown",
        "active_streams": "unknown", 
        "loops_detected": "unknown",
        "average_response_time": "unknown",
        "note": "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ"
    }


def _get_loop_prevention_recommendations(prompt: str, phrase_counts: dict) -> list[str]:
    """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—é –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è."""
    recommendations = []
    
    if len(prompt.split()) < 10:
        recommendations.append("–ü—Ä–æ–º–ø—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π - –¥–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
    
    if phrase_counts:
        recommendations.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã: {list(phrase_counts.keys())}")
        recommendations.append("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å repeat_penalty –¥–æ 1.15-1.2")
    
    if "–ø–æ–¥–æ–∂–¥–∏—Ç–µ" in phrase_counts or "–ø—Ä–æ–≤–µ—Ä–∏—Ç—å" in phrase_counts:
        recommendations.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ñ—Ä–∞–∑—ã, —á–∞—Å—Ç–æ –≤—ã–∑—ã–≤–∞—é—â–∏–µ –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏–µ")
        recommendations.append("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç")
    
    if not recommendations:
        recommendations.append("–ü—Ä–æ–º–ø—Ç –≤—ã–≥–ª—è–¥–∏—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
    
    return recommendations 