"""Ollama-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API —Ä–æ—É—Ç–µ—Ä."""

import os
import time
from collections.abc import AsyncIterator

import structlog
from fastapi import APIRouter, Depends, HTTPException, Request
from fastapi.responses import StreamingResponse

from ...config import get_settings
from ...domain.models import ChatMessage
from ...domain.protocols import LlamaServiceProtocol

router = APIRouter()
logger = structlog.get_logger(__name__)


async def get_llama_service(request: Request) -> LlamaServiceProtocol:
    """–ü–æ–ª—É—á–∏—Ç—å LLama —Å–µ—Ä–≤–∏—Å –∏–∑ DI –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞."""
    return request.app.state.container.llama_service()


def get_available_model_name() -> str:
    """–ü–æ–ª—É—á–∏—Ç—å –∏–º—è –¥–æ—Å—Ç—É–ø–Ω–æ–π –º–æ–¥–µ–ª–∏."""
    settings = get_settings()
    return os.path.basename(str(settings.model_path)).replace(".gguf", "")


def validate_model_name(requested_model: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏."""
    available = get_available_model_name()

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å (—É–±–∏—Ä–∞–µ–º —Ç–µ–≥–∏)
    normalized = requested_model.replace(":latest", "").replace(":stable", "")

    # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    logger.debug(
        "–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏",
        requested=requested_model,
        normalized=normalized,
        available=available
    )

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    variants = [
        normalized == available,  # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π
        requested_model == available,  # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–π
        normalized == "llama-cpp",
        normalized == "llamacpp",
        available.replace(".gguf", "") == normalized,  # –ë–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
        available.startswith(normalized),  # –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å
        normalized in available,  # –°–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ –∏–º–µ–Ω–∏
    ]

    result = any(variants)

    if not result:
        logger.warning(
            "–ú–æ–¥–µ–ª—å –Ω–µ –ø—Ä–æ—à–ª–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é",
            requested=requested_model,
            normalized=normalized,
            available=available,
            variants_checked=len(variants)
        )

    return result


@router.get("/api/tags")
async def ollama_list_models():
    """Ollama: –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
    settings = get_settings()
    model_name = get_available_model_name()
    # –ö–ª–∏–µ–Ω—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è—é—Ç :latest, –ø–æ—ç—Ç–æ–º—É –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–∞–∑—É —Å —Ç–µ–≥–æ–º
    model_with_tag = f"{model_name}:latest"

    return {
        "models": [
            {
                "name": model_with_tag,
                "model": model_with_tag,
                "modified_at": "2024-01-01T00:00:00.000Z",
                "size": os.path.getsize(settings.model_path) if settings.model_path.exists() else 0,
                "digest": "sha256:dummy",
                "details": {
                    "parent_model": "",
                    "format": "gguf",
                    "family": "llama",
                    "families": ["llama"],
                    "parameter_size": "7B",
                    "quantization_level": "Q4_K_M"
                }
            }
        ]
    }


@router.get("/api/ps")
async def ollama_list_running(
    llama_service: LlamaServiceProtocol = Depends(get_llama_service)
):
    """Ollama: –°–ø–∏—Å–æ–∫ –∑–∞–ø—É—â–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
    settings = get_settings()
    model_name = get_available_model_name()

    is_ready = await llama_service.is_ready()
    if is_ready:
        return {
            "models": [
                {
                    "name": model_name,
                    "model": model_name,
                    "size": os.path.getsize(settings.model_path) if settings.model_path.exists() else 0,
                    "digest": "sha256:dummy",
                    "details": {
                        "parent_model": "",
                        "format": "gguf",
                        "family": "llama",
                        "families": ["llama"],
                        "parameter_size": "7B",
                        "quantization_level": "Q4_K_M"
                    },
                    "expires_at": "0001-01-01T00:00:00Z",
                    "size_vram": 0
                }
            ]
        }
    else:
        return {"models": []}


@router.get("/api/version")
async def ollama_version():
    """Ollama: –í–µ—Ä—Å–∏—è API."""
    return {"version": "0.1.0"}


@router.post("/api/show")
async def ollama_show_model(request_data: dict):
    """Ollama: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏."""
    settings = get_settings()
    model_name = request_data.get("name", "")

    if not validate_model_name(model_name):
        available_model = get_available_model_name()
        raise HTTPException(
            status_code=400,
            detail=f"Model '{model_name}' not found. Available: '{available_model}'"
        )

    return {
        "modelfile": f"FROM {settings.model_path}",
        "parameters": f"temperature {settings.temperature}",
        "template": "{{ .System }}\n{{ .Prompt }}",
        "details": {
            "parent_model": "",
            "format": "gguf",
            "family": "llama",
            "families": ["llama"],
            "parameter_size": "7B",
            "quantization_level": "Q4_K_M"
        },
        "model_info": {
            "general.architecture": "llama",
            "general.file_type": 15,
            "general.parameter_count": 7000000000,
            "general.quantization_version": 2,
            "llama.attention.head_count": 32,
            "llama.block_count": 32,
            "llama.context_length": settings.n_ctx,
            "llama.embedding_length": 4096,
            "llama.feed_forward_length": 11008,
            "llama.rope.freq_base": 10000,
            "tokenizer.ggml.model": "llama"
        }
    }


@router.post("/api/generate")
async def ollama_generate(
    request_data: dict,
    llama_service: LlamaServiceProtocol = Depends(get_llama_service)
):
    """Ollama: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞."""
    model_name = request_data.get("model", "")
    prompt = request_data.get("prompt", "")
    stream = request_data.get("stream", False)

    if not validate_model_name(model_name):
        available_model = get_available_model_name()
        raise HTTPException(
            status_code=400,
            detail=f"Model '{model_name}' not found. Available: '{available_model}'"
        )

    if not prompt:
        raise HTTPException(status_code=400, detail="Prompt is required")

    try:
        from ...domain.models import TextCompletionRequest

        completion_request = TextCompletionRequest(
            prompt=prompt,
            model=model_name,
            max_tokens=get_settings().get_effective_max_tokens(request_data.get("max_tokens")),
            temperature=request_data.get("temperature", get_settings().temperature),
            top_p=request_data.get("top_p", get_settings().top_p),
            top_k=request_data.get("top_k", get_settings().top_k),
            repeat_penalty=request_data.get("repeat_penalty", get_settings().repeat_penalty),
            seed=request_data.get("seed", None),
            stream=stream
        )

        if stream:
            return StreamingResponse(
                _ollama_generate_stream(llama_service, completion_request, model_name),
                media_type="application/x-ndjson",
                headers={
                    "Content-Type": "application/x-ndjson; charset=utf-8",
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Access-Control-Allow-Origin": "*",
                    "X-Accel-Buffering": "no"  # –û—Ç–∫–ª—é—á–∞–µ–º –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—é nginx
                }
            )
        else:
            response = await llama_service.text_completion(completion_request)
            response_text = response.choices[0].text if response.choices else ""

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç –Ω–µ –ø—É—Å—Ç–æ–π
            if not response_text.strip():
                logger.warning("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è ollama generate", model=model_name, prompt=prompt[:100])
                response_text = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —Å–º–æ–≥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å."

            return {
                "model": model_name,
                "created_at": f"{time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())}",
                "response": response_text,
                "done": True,
                "context": [],
                "total_duration": 1000000000,
                "load_duration": 100000000,
                "prompt_eval_count": response.usage.prompt_tokens if response.usage else 0,
                "prompt_eval_duration": 500000000,
                "eval_count": response.usage.completion_tokens if response.usage else 0,
                "eval_duration": 400000000
            }

    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ Ollama generate", error=str(e), exc_info=True)
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")


@router.post("/api/chat")
async def ollama_chat(
    request_data: dict,
    llama_service: LlamaServiceProtocol = Depends(get_llama_service)
):
    """Ollama: –ß–∞—Ç completion."""
    model_name = request_data.get("model", "")
    messages = request_data.get("messages", [])
    stream = request_data.get("stream", False)

    logger.info("üîç –í—Ö–æ–¥—è—â–∏–π –∑–∞–ø—Ä–æ—Å ollama chat", 
                model=model_name, 
                messages_count=len(messages),
                stream=stream,
                first_message=messages[0] if messages else None)

    if not validate_model_name(model_name):
        available_model = get_available_model_name()
        raise HTTPException(
            status_code=400,
            detail=f"Model '{model_name}' not found. Available: '{available_model}'"
        )

    if not messages:
        raise HTTPException(status_code=400, detail="Messages are required")

    try:
        from ...domain.models import ChatCompletionRequest

        chat_messages = [ChatMessage(**msg) for msg in messages]
        completion_request = ChatCompletionRequest(
            messages=chat_messages,
            model=model_name,
            max_tokens=get_settings().get_effective_max_tokens(request_data.get("max_tokens")),
            temperature=request_data.get("temperature", get_settings().temperature),
            top_p=request_data.get("top_p", get_settings().top_p),
            top_k=request_data.get("top_k", get_settings().top_k),
            repeat_penalty=request_data.get("repeat_penalty", get_settings().repeat_penalty),
            seed=request_data.get("seed", None),
            stream=stream
        )

        if stream:
            return StreamingResponse(
                _ollama_chat_stream(llama_service, completion_request, model_name),
                media_type="application/x-ndjson",
                headers={
                    "Content-Type": "application/x-ndjson; charset=utf-8",
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Access-Control-Allow-Origin": "*",
                    "X-Accel-Buffering": "no"  # –û—Ç–∫–ª—é—á–∞–µ–º –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—é nginx
                }
            )
        else:
            logger.info("üß† –ó–∞–ø—Ä–æ—Å –Ω–µ-—Å—Ç—Ä–∏–º–∏–Ω–≥ —á–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", model=model_name)
            response = await llama_service.chat_completion(completion_request)
            response_text = response.choices[0].message.content if response.choices else ""

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç –Ω–µ –ø—É—Å—Ç–æ–π
            if not response_text or not response_text.strip():
                logger.warning("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è ollama chat", 
                             model=model_name, 
                             messages_preview=str(messages)[:200])
                response_text = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —Å–º–æ–≥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à –∑–∞–ø—Ä–æ—Å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."

            result = {
                "model": model_name,
                "created_at": f"{time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())}",
                "message": {
                    "role": "assistant",
                    "content": response_text,
                    "images": None
                },
                "done": True,
                "total_duration": 1000000000,
                "load_duration": 100000000,
                "prompt_eval_count": response.usage.prompt_tokens if response.usage else 0,
                "prompt_eval_duration": 500000000,
                "eval_count": response.usage.completion_tokens if response.usage else 0,
                "eval_duration": 400000000
            }

            logger.info("‚úÖ Ollama chat –æ—Ç–≤–µ—Ç –≥–æ—Ç–æ–≤", 
                       content_length=len(response_text),
                       preview=response_text[:100])
            
            return result

    except Exception as e:
        logger.error("‚ùå –û—à–∏–±–∫–∞ Ollama chat", error=str(e), exc_info=True)
        raise HTTPException(status_code=500, detail=f"Chat completion failed: {str(e)}")


async def _ollama_generate_stream(
    llama_service: LlamaServiceProtocol,
    request: "TextCompletionRequest",
    model_name: str
) -> AsyncIterator[str]:
    """–°—Ç—Ä–∏–º–∏–Ω–≥ –¥–ª—è Ollama generate."""
    import json

    logger.info("üîÑ –ó–∞–ø—É—Å–∫ ollama generate stream", model=model_name)
    
    try:
        chunk_count = 0
        total_content = ""
        
        async for chunk in llama_service.text_completion_stream(request):
            chunk_count += 1
            
            if "choices" in chunk and chunk["choices"]:
                choice = chunk["choices"][0]
                content = choice.get("text", "") or choice.get("delta", {}).get("content", "")
                
                if content:  # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç
                    total_content += content
                    response_chunk = {
                        "model": model_name,
                        "created_at": f"{time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())}",
                        "response": content,
                        "done": False
                    }
                    yield f"{json.dumps(response_chunk, ensure_ascii=False)}\n"

        # –§–∏–Ω–∞–ª—å–Ω—ã–π chunk
        final_chunk = {
            "model": model_name,
            "created_at": f"{time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())}",
            "response": "",
            "done": True,
            "total_duration": 1000000000,
            "load_duration": 100000000,
            "prompt_eval_count": 0,
            "prompt_eval_duration": 500000000,
            "eval_count": 0,
            "eval_duration": 400000000
        }
        yield f"{json.dumps(final_chunk, ensure_ascii=False)}\n"
        
        logger.info("‚úÖ Ollama generate stream –∑–∞–≤–µ—Ä—à–µ–Ω", 
                   chunks=chunk_count, 
                   total_length=len(total_content))
        
    except Exception as e:
        logger.error("‚ùå –û—à–∏–±–∫–∞ ollama generate stream", error=str(e), exc_info=True)
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ Ollama
        error_chunk = {
            "model": model_name,
            "created_at": f"{time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())}",
            "response": f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}",
            "done": True,
            "error": str(e)
        }
        yield f"{json.dumps(error_chunk, ensure_ascii=False)}\n"


async def _ollama_chat_stream(
    llama_service: LlamaServiceProtocol,
    request: "ChatCompletionRequest",
    model_name: str
) -> AsyncIterator[str]:
    """–°—Ç—Ä–∏–º–∏–Ω–≥ –¥–ª—è Ollama chat."""
    import json

    logger.info("üîÑ –ó–∞–ø—É—Å–∫ ollama chat stream", model=model_name)
    
    try:
        chunk_count = 0
        total_content = ""
        
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –∏—Ç–µ—Ä–∞—Ü–∏—é –ø–æ chat_completion_stream")
        
        async for chunk in llama_service.chat_completion_stream(request):
            chunk_count += 1
            logger.debug(f"üì¶ –ü–æ–ª—É—á–µ–Ω chunk #{chunk_count}", chunk_keys=list(chunk.keys()) if chunk else None)
            
            if "choices" in chunk and chunk["choices"]:
                delta = chunk["choices"][0].get("delta", {})
                content = delta.get("content", "")
                logger.debug(f"üí¨ Content –∏–∑ chunk: {repr(content)}")

                if content:  # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç
                    total_content += content
                    response_chunk = {
                        "model": model_name,
                        "created_at": f"{time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())}",
                        "message": {
                            "role": "assistant",
                            "content": content,
                            "images": None
                        },
                        "done": False
                    }
                    chunk_json = f"{json.dumps(response_chunk, ensure_ascii=False)}\n"
                    logger.debug(f"üì§ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º chunk: {chunk_json[:100]}...")
                    yield chunk_json

        # –§–∏–Ω–∞–ª—å–Ω—ã–π chunk
        final_chunk = {
            "model": model_name,
            "created_at": f"{time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())}",
            "message": {
                "role": "assistant",
                "content": "",
                "images": None
            },
            "done": True,
            "total_duration": 1000000000,
            "load_duration": 100000000,
            "prompt_eval_count": 0,
            "prompt_eval_duration": 500000000,
            "eval_count": 0,
            "eval_duration": 400000000
        }
        final_json = f"{json.dumps(final_chunk, ensure_ascii=False)}\n"
        logger.debug("üì§ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π chunk")
        yield final_json
        
        logger.debug("‚úÖ Ollama chat stream –∑–∞–≤–µ—Ä—à–µ–Ω", 
                   chunks=chunk_count, 
                   total_length=len(total_content))
        
    except Exception as e:
        logger.error("‚ùå –û—à–∏–±–∫–∞ ollama chat stream", error=str(e), exc_info=True)
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ Ollama
        error_chunk = {
            "model": model_name,
            "created_at": f"{time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())}",
            "message": {
                "role": "assistant",
                "content": f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}",
                "images": None
            },
            "done": True,
            "error": str(e)
        }
        yield f"{json.dumps(error_chunk, ensure_ascii=False)}\n"
