# ü¶ô llamacpp-server

–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –¥–ª—è llama.cpp —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å OpenAI API, Ollama –∏ Open WebUI.

## ‚ú® –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- üöÄ **GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ** —Å CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
- üîÑ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–æ—Ç–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞** - –Ω–∏–∫–∞–∫–∏—Ö –æ—à–∏–±–æ–∫ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è
- üåê **–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è API —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å**:
  - OpenAI API (`/v1/chat/completions`, `/v1/completions`)
  - Ollama API (`/api/chat`, `/api/generate`, `/api/tags`)
  - Open WebUI API (`/api/*`)
- üì° **–°—Ç—Ä–∏–º–∏–Ω–≥** - —Ç–æ–∫–µ–Ω –∑–∞ —Ç–æ–∫–µ–Ω–æ–º –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- üß† **–£–º–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–µ–π** —á–∞—Ç–∞
- üêç **Python 2025 —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã** - uv, ruff, mypy, pytest
- üèóÔ∏è **Clean Architecture** - —Å–ª–æ–∏—Å—Ç–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
```bash
# –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
git clone <repo-url>
cd llamacpp-server

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
uv sync --extra cpu

# –î–ª—è GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∏ - —Å–º. docs/GPU_SETUP.md
```

### –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
```bash
# –†–µ–∂–∏–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ (–±–µ–∑ –º–æ–¥–µ–ª–∏)
LLAMACPP_DEV_MODE=true uv run python -m llamacpp_server.main

# –° —Ä–µ–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é
LLAMACPP_MODEL_PATH=models/model.gguf uv run python -m llamacpp_server.main

# –° GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º
LLAMACPP_N_GPU_LAYERS=20 uv run python -m llamacpp_server.main
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã
```bash
# –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
curl http://localhost:8090/api/tags

# Chat completion
curl -X POST http://localhost:8090/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "llama", "messages": [{"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç!"}]}'
```

## üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

- [üèóÔ∏è –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞](docs/INSTALLATION.md)
- [üéÆ GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∞](docs/GPU_SETUP.md)
- [üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è](docs/CONFIGURATION.md)
- [üì° API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](docs/API.md)
- [üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ](docs/TESTING.md)
- [üìã Changelog](CHANGELOG.md)

## ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

–û—Å–Ω–æ–≤–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è:

```bash
# === –ú–æ–¥–µ–ª—å ===
LLAMACPP_MODEL_PATH=models/model.gguf
LLAMACPP_DEV_MODE=false

# === GPU ===
LLAMACPP_N_GPU_LAYERS=20    # 0=CPU, -1=–≤—Å–µ —Å–ª–æ–∏ –Ω–∞ GPU
LLAMACPP_MAIN_GPU=0
LLAMACPP_TENSOR_SPLIT="0.7,0.3"

# === –ö–æ–Ω—Ç–µ–∫—Å—Ç ===
LLAMACPP_N_CTX=4096
LLAMACPP_MAX_HISTORY_TOKENS=3000
LLAMACPP_CONTEXT_RESERVE_TOKENS=1000

# === –°–µ—Ä–≤–µ—Ä ===
LLAMACPP_HOST=0.0.0.0
LLAMACPP_PORT=8090
```

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
llamacpp_server/
‚îú‚îÄ‚îÄ config/          # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚îú‚îÄ‚îÄ domain/          # –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã
‚îú‚îÄ‚îÄ infrastructure/  # –í–Ω–µ—à–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (DI)
‚îú‚îÄ‚îÄ llama/          # –õ–æ–≥–∏–∫–∞ —Ä–∞–±–æ—Ç—ã —Å llama.cpp
‚îú‚îÄ‚îÄ prompts/        # –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
‚îî‚îÄ‚îÄ web/            # FastAPI —Ä–æ—É—Ç–µ—Ä—ã –∏ API
```

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
# –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤
uv run pytest

# –° –ø–æ–∫—Ä—ã—Ç–∏–µ–º –∫–æ–¥–∞
uv run pytest --cov=llamacpp_server

# –¢–æ–ª—å–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã
uv run pytest tests/test_config.py
```

## üîß –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å dev –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
uv sync --extra all

# –õ–∏–Ω—Ç–∏–Ω–≥ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
uv run ruff check .
uv run ruff format .

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤
uv run mypy llamacpp_server/
```

## üìä –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

| –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è | –°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ |
|--------------|-------------------|
| CPU —Ç–æ–ª—å–∫–æ | ~2-5 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫ |
| RTX 3060 | ~15-25 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫ |
| RTX 4090 | ~50-80 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫ |

## ü§ù –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å

- **Python**: 3.11+
- **–û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã**: Linux, macOS, Windows
- **GPU**: NVIDIA CUDA 12.1+, AMD ROCm, Apple Metal
- **API –∫–ª–∏–µ–Ω—Ç—ã**: OpenAI SDK, Ollama, Open WebUI, LangChain

## üìÑ –õ–∏—Ü–µ–Ω–∑–∏—è

MIT License - —Å–º. [LICENSE](LICENSE)

## üôè –ë–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏

- [llama.cpp](https://github.com/ggerganov/llama.cpp) - –æ—Å–Ω–æ–≤–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) - Python –±–∏–Ω–¥–∏–Ω–≥–∏
- [FastAPI](https://fastapi.tiangolo.com/) - –≤–µ–± —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ 